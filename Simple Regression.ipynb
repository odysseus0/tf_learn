{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.12.0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "tf.VERSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = 50\n",
    "xs = np.linspace(-10, 12, num=steps)[:, None]\n",
    "ys = -xs ** 2 + 3 * xs + 1 + 5 * np.random.randn(steps, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((50, 1), (50, 1))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xs.shape, ys.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0xb38309da0>]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(xs, ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = np.random.randn(2, 1) / np.sqrt(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = np.random.randn(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs_t = tf.constant(xs)\n",
    "ys_t = tf.constant(ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor 'Const:0' shape=(50, 1) dtype=float64>,\n",
       " <tf.Tensor 'Const_1:0' shape=(50, 1) dtype=float64>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xs_t, ys_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = tf.Variable(W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = tf.Variable(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "ys_pred = tf.concat([xs_t, tf.pow(xs_t, 2)], axis = 1) @ W + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'add:0' shape=(50, 1) dtype=float64>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ys_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'mean_squared_error/value:0' shape=() dtype=float32>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = tf.losses.mean_squared_error(ys_t, ys_pred)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = tf.train.AdamOptimizer(learning_rate=1e-2).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0xb385d92b0>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "preds = sess.run(ys_pred)\n",
    "plt.figure()\n",
    "plt.plot(xs, preds, label='prediction')\n",
    "plt.plot(xs, ys, label='target')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current training loss is 6904.27001953125\n",
      "Current training loss is 6806.359375\n",
      "Current training loss is 6709.24462890625\n",
      "Current training loss is 6612.9384765625\n",
      "Current training loss is 6517.45166015625\n",
      "Current training loss is 6422.7978515625\n",
      "Current training loss is 6328.98779296875\n",
      "Current training loss is 6236.033203125\n",
      "Current training loss is 6143.94384765625\n",
      "Current training loss is 6052.72900390625\n",
      "Current training loss is 5962.39990234375\n",
      "Current training loss is 5872.96484375\n",
      "Current training loss is 5784.4296875\n",
      "Current training loss is 5696.80615234375\n",
      "Current training loss is 5610.09814453125\n",
      "Current training loss is 5524.3125\n",
      "Current training loss is 5439.4560546875\n",
      "Current training loss is 5355.5322265625\n",
      "Current training loss is 5272.54736328125\n",
      "Current training loss is 5190.50439453125\n",
      "Current training loss is 5109.40625\n",
      "Current training loss is 5029.25537109375\n",
      "Current training loss is 4950.0556640625\n",
      "Current training loss is 4871.806640625\n",
      "Current training loss is 4794.5087890625\n",
      "Current training loss is 4718.16357421875\n",
      "Current training loss is 4642.77001953125\n",
      "Current training loss is 4568.32763671875\n",
      "Current training loss is 4494.83447265625\n",
      "Current training loss is 4422.28955078125\n",
      "Current training loss is 4350.68994140625\n",
      "Current training loss is 4280.03173828125\n",
      "Current training loss is 4210.31298828125\n",
      "Current training loss is 4141.5302734375\n",
      "Current training loss is 4073.6787109375\n",
      "Current training loss is 4006.752685546875\n",
      "Current training loss is 3940.749267578125\n",
      "Current training loss is 3875.662353515625\n",
      "Current training loss is 3811.486572265625\n",
      "Current training loss is 3748.21533203125\n",
      "Current training loss is 3685.843017578125\n",
      "Current training loss is 3624.3623046875\n",
      "Current training loss is 3563.767822265625\n",
      "Current training loss is 3504.05126953125\n",
      "Current training loss is 3445.2060546875\n",
      "Current training loss is 3387.224609375\n",
      "Current training loss is 3330.099609375\n",
      "Current training loss is 3273.822509765625\n",
      "Current training loss is 3218.385498046875\n",
      "Current training loss is 3163.78076171875\n",
      "Current training loss is 3110.0\n",
      "Current training loss is 3057.034423828125\n",
      "Current training loss is 3004.875244140625\n",
      "Current training loss is 2953.514892578125\n",
      "Current training loss is 2902.943359375\n",
      "Current training loss is 2853.153076171875\n",
      "Current training loss is 2804.1337890625\n",
      "Current training loss is 2755.877685546875\n",
      "Current training loss is 2708.375244140625\n",
      "Current training loss is 2661.617431640625\n",
      "Current training loss is 2615.595458984375\n",
      "Current training loss is 2570.300048828125\n",
      "Current training loss is 2525.721923828125\n",
      "Current training loss is 2481.852294921875\n",
      "Current training loss is 2438.681884765625\n",
      "Current training loss is 2396.201904296875\n",
      "Current training loss is 2354.403076171875\n",
      "Current training loss is 2313.276123046875\n",
      "Current training loss is 2272.812255859375\n",
      "Current training loss is 2233.002685546875\n",
      "Current training loss is 2193.838134765625\n",
      "Current training loss is 2155.31005859375\n",
      "Current training loss is 2117.409423828125\n",
      "Current training loss is 2080.127685546875\n",
      "Current training loss is 2043.4561767578125\n",
      "Current training loss is 2007.3857421875\n",
      "Current training loss is 1971.9085693359375\n",
      "Current training loss is 1937.015625\n",
      "Current training loss is 1902.69873046875\n",
      "Current training loss is 1868.9495849609375\n",
      "Current training loss is 1835.76025390625\n",
      "Current training loss is 1803.1221923828125\n",
      "Current training loss is 1771.02734375\n",
      "Current training loss is 1739.4678955078125\n",
      "Current training loss is 1708.435791015625\n",
      "Current training loss is 1677.9237060546875\n",
      "Current training loss is 1647.92333984375\n",
      "Current training loss is 1618.4276123046875\n",
      "Current training loss is 1589.4290771484375\n",
      "Current training loss is 1560.919921875\n",
      "Current training loss is 1532.89306640625\n",
      "Current training loss is 1505.3411865234375\n",
      "Current training loss is 1478.257568359375\n",
      "Current training loss is 1451.6346435546875\n",
      "Current training loss is 1425.4661865234375\n",
      "Current training loss is 1399.74462890625\n",
      "Current training loss is 1374.4637451171875\n",
      "Current training loss is 1349.61669921875\n",
      "Current training loss is 1325.197021484375\n",
      "Current training loss is 1301.1981201171875\n",
      "Current training loss is 1277.61376953125\n",
      "Current training loss is 1254.4375\n",
      "Current training loss is 1231.6634521484375\n",
      "Current training loss is 1209.28515625\n",
      "Current training loss is 1187.296875\n",
      "Current training loss is 1165.6925048828125\n",
      "Current training loss is 1144.4661865234375\n",
      "Current training loss is 1123.6119384765625\n",
      "Current training loss is 1103.12451171875\n",
      "Current training loss is 1082.997802734375\n",
      "Current training loss is 1063.2265625\n",
      "Current training loss is 1043.8052978515625\n",
      "Current training loss is 1024.7283935546875\n",
      "Current training loss is 1005.9906616210938\n",
      "Current training loss is 987.5868530273438\n",
      "Current training loss is 969.51171875\n",
      "Current training loss is 951.7600708007812\n",
      "Current training loss is 934.3268432617188\n",
      "Current training loss is 917.2071533203125\n",
      "Current training loss is 900.3960571289062\n",
      "Current training loss is 883.8885498046875\n",
      "Current training loss is 867.6799926757812\n",
      "Current training loss is 851.7655029296875\n",
      "Current training loss is 836.140625\n",
      "Current training loss is 820.8003540039062\n",
      "Current training loss is 805.740478515625\n",
      "Current training loss is 790.9562377929688\n",
      "Current training loss is 776.4434204101562\n",
      "Current training loss is 762.1973266601562\n",
      "Current training loss is 748.2139892578125\n",
      "Current training loss is 734.4888916015625\n",
      "Current training loss is 721.0178833007812\n",
      "Current training loss is 707.7967529296875\n",
      "Current training loss is 694.8214721679688\n",
      "Current training loss is 682.0877685546875\n",
      "Current training loss is 669.5919189453125\n",
      "Current training loss is 657.329833984375\n",
      "Current training loss is 645.2975463867188\n",
      "Current training loss is 633.4912719726562\n",
      "Current training loss is 621.9072265625\n",
      "Current training loss is 610.5416259765625\n",
      "Current training loss is 599.3906860351562\n",
      "Current training loss is 588.450927734375\n",
      "Current training loss is 577.718505859375\n",
      "Current training loss is 567.1901245117188\n",
      "Current training loss is 556.862060546875\n",
      "Current training loss is 546.7310180664062\n",
      "Current training loss is 536.7933959960938\n",
      "Current training loss is 527.0460205078125\n",
      "Current training loss is 517.4854736328125\n",
      "Current training loss is 508.10845947265625\n",
      "Current training loss is 498.9117431640625\n",
      "Current training loss is 489.8922119140625\n",
      "Current training loss is 481.0466613769531\n",
      "Current training loss is 472.3720703125\n",
      "Current training loss is 463.865234375\n",
      "Current training loss is 455.5233459472656\n",
      "Current training loss is 447.34326171875\n",
      "Current training loss is 439.3221435546875\n",
      "Current training loss is 431.4571533203125\n",
      "Current training loss is 423.7453308105469\n",
      "Current training loss is 416.1839599609375\n",
      "Current training loss is 408.7702941894531\n",
      "Current training loss is 401.5016174316406\n",
      "Current training loss is 394.3752746582031\n",
      "Current training loss is 387.3885803222656\n",
      "Current training loss is 380.5389404296875\n",
      "Current training loss is 373.8238830566406\n",
      "Current training loss is 367.2408752441406\n",
      "Current training loss is 360.7873840332031\n",
      "Current training loss is 354.4609680175781\n",
      "Current training loss is 348.25927734375\n",
      "Current training loss is 342.17999267578125\n",
      "Current training loss is 336.2207336425781\n",
      "Current training loss is 330.3792419433594\n",
      "Current training loss is 324.65325927734375\n",
      "Current training loss is 319.0406494140625\n",
      "Current training loss is 313.53912353515625\n",
      "Current training loss is 308.1466369628906\n",
      "Current training loss is 302.8610534667969\n",
      "Current training loss is 297.6802978515625\n",
      "Current training loss is 292.6023254394531\n",
      "Current training loss is 287.62518310546875\n",
      "Current training loss is 282.74688720703125\n",
      "Current training loss is 277.9654846191406\n",
      "Current training loss is 273.2791442871094\n",
      "Current training loss is 268.6859436035156\n",
      "Current training loss is 264.18402099609375\n",
      "Current training loss is 259.7716979980469\n",
      "Current training loss is 255.44708251953125\n",
      "Current training loss is 251.20852661132812\n",
      "Current training loss is 247.05429077148438\n",
      "Current training loss is 242.98269653320312\n",
      "Current training loss is 238.9921112060547\n",
      "Current training loss is 235.0808868408203\n",
      "Current training loss is 231.24749755859375\n",
      "Current training loss is 227.49034118652344\n",
      "Current training loss is 223.80789184570312\n",
      "Current training loss is 220.1986541748047\n",
      "Current training loss is 216.66119384765625\n",
      "Current training loss is 213.19400024414062\n",
      "Current training loss is 209.7957000732422\n",
      "Current training loss is 206.46490478515625\n",
      "Current training loss is 203.20022583007812\n",
      "Current training loss is 200.0003662109375\n",
      "Current training loss is 196.8639373779297\n",
      "Current training loss is 193.7897186279297\n",
      "Current training loss is 190.77642822265625\n",
      "Current training loss is 187.8228302001953\n",
      "Current training loss is 184.92767333984375\n",
      "Current training loss is 182.08985900878906\n",
      "Current training loss is 179.30812072753906\n",
      "Current training loss is 176.5813446044922\n",
      "Current training loss is 173.90843200683594\n",
      "Current training loss is 171.28822326660156\n",
      "Current training loss is 168.71974182128906\n",
      "Current training loss is 166.2018280029297\n",
      "Current training loss is 163.73350524902344\n",
      "Current training loss is 161.31375122070312\n",
      "Current training loss is 158.9415283203125\n",
      "Current training loss is 156.6159210205078\n",
      "Current training loss is 154.3359375\n",
      "Current training loss is 152.10067749023438\n",
      "Current training loss is 149.90921020507812\n",
      "Current training loss is 147.76058959960938\n",
      "Current training loss is 145.65399169921875\n",
      "Current training loss is 143.58853149414062\n",
      "Current training loss is 141.5634002685547\n",
      "Current training loss is 139.57772827148438\n",
      "Current training loss is 137.63072204589844\n",
      "Current training loss is 135.7216033935547\n",
      "Current training loss is 133.84959411621094\n",
      "Current training loss is 132.01390075683594\n",
      "Current training loss is 130.21380615234375\n",
      "Current training loss is 128.44859313964844\n",
      "Current training loss is 126.7175064086914\n",
      "Current training loss is 125.01991271972656\n",
      "Current training loss is 123.35508728027344\n",
      "Current training loss is 121.72237396240234\n",
      "Current training loss is 120.12107849121094\n",
      "Current training loss is 118.55064392089844\n",
      "Current training loss is 117.01034545898438\n",
      "Current training loss is 115.49966430664062\n",
      "Current training loss is 114.01792907714844\n",
      "Current training loss is 112.56459045410156\n",
      "Current training loss is 111.13905334472656\n",
      "Current training loss is 109.74073791503906\n",
      "Current training loss is 108.369140625\n",
      "Current training loss is 107.02368927001953\n",
      "Current training loss is 105.70386505126953\n",
      "Current training loss is 104.40914154052734\n",
      "Current training loss is 103.13905334472656\n",
      "Current training loss is 101.89302825927734\n",
      "Current training loss is 100.67066192626953\n",
      "Current training loss is 99.47144317626953\n",
      "Current training loss is 98.29490661621094\n",
      "Current training loss is 97.14060974121094\n",
      "Current training loss is 96.00810241699219\n",
      "Current training loss is 94.89694213867188\n",
      "Current training loss is 93.8067398071289\n",
      "Current training loss is 92.73702239990234\n",
      "Current training loss is 91.68746185302734\n",
      "Current training loss is 90.65755462646484\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current training loss is 89.64701080322266\n",
      "Current training loss is 88.6553955078125\n",
      "Current training loss is 87.6823501586914\n",
      "Current training loss is 86.72750091552734\n",
      "Current training loss is 85.79048919677734\n",
      "Current training loss is 84.8709945678711\n",
      "Current training loss is 83.96864318847656\n",
      "Current training loss is 83.08311462402344\n",
      "Current training loss is 82.21405792236328\n",
      "Current training loss is 81.36119079589844\n",
      "Current training loss is 80.52417755126953\n",
      "Current training loss is 79.70272064208984\n",
      "Current training loss is 78.89649200439453\n",
      "Current training loss is 78.10523223876953\n",
      "Current training loss is 77.32862854003906\n",
      "Current training loss is 76.56640625\n",
      "Current training loss is 75.81831359863281\n",
      "Current training loss is 75.08403015136719\n",
      "Current training loss is 74.36333465576172\n",
      "Current training loss is 73.65595245361328\n",
      "Current training loss is 72.96161651611328\n",
      "Current training loss is 72.28009796142578\n",
      "Current training loss is 71.61113739013672\n",
      "Current training loss is 70.95452117919922\n",
      "Current training loss is 70.31000518798828\n",
      "Current training loss is 69.67735290527344\n",
      "Current training loss is 69.05635833740234\n",
      "Current training loss is 68.4467544555664\n",
      "Current training loss is 67.84838104248047\n",
      "Current training loss is 67.2610092163086\n",
      "Current training loss is 66.68443298339844\n",
      "Current training loss is 66.11845397949219\n",
      "Current training loss is 65.56285858154297\n",
      "Current training loss is 65.01747131347656\n",
      "Current training loss is 64.48208618164062\n",
      "Current training loss is 63.95653533935547\n",
      "Current training loss is 63.44063186645508\n",
      "Current training loss is 62.93418502807617\n",
      "Current training loss is 62.43703079223633\n",
      "Current training loss is 61.949012756347656\n",
      "Current training loss is 61.46992874145508\n",
      "Current training loss is 60.9996452331543\n",
      "Current training loss is 60.537967681884766\n",
      "Current training loss is 60.08477783203125\n",
      "Current training loss is 59.63990020751953\n",
      "Current training loss is 59.20316696166992\n",
      "Current training loss is 58.77445602416992\n",
      "Current training loss is 58.35361099243164\n",
      "Current training loss is 57.940486907958984\n",
      "Current training loss is 57.53494644165039\n",
      "Current training loss is 57.13684844970703\n",
      "Current training loss is 56.74605941772461\n",
      "Current training loss is 56.362449645996094\n",
      "Current training loss is 55.985877990722656\n",
      "Current training loss is 55.61623764038086\n",
      "Current training loss is 55.25337600708008\n",
      "Current training loss is 54.89720153808594\n",
      "Current training loss is 54.547576904296875\n",
      "Current training loss is 54.20437240600586\n",
      "Current training loss is 53.86749267578125\n",
      "Current training loss is 53.53681564331055\n",
      "Current training loss is 53.21223449707031\n",
      "Current training loss is 52.89362335205078\n",
      "Current training loss is 52.58088684082031\n",
      "Current training loss is 52.27394104003906\n",
      "Current training loss is 51.97264099121094\n",
      "Current training loss is 51.676902770996094\n",
      "Current training loss is 51.38664245605469\n",
      "Current training loss is 51.10173797607422\n",
      "Current training loss is 50.82210922241211\n",
      "Current training loss is 50.547645568847656\n",
      "Current training loss is 50.27827835083008\n",
      "Current training loss is 50.0139045715332\n",
      "Current training loss is 49.75442886352539\n",
      "Current training loss is 49.49976348876953\n",
      "Current training loss is 49.24983215332031\n",
      "Current training loss is 49.004539489746094\n",
      "Current training loss is 48.763816833496094\n",
      "Current training loss is 48.527557373046875\n",
      "Current training loss is 48.29572296142578\n",
      "Current training loss is 48.06819534301758\n",
      "Current training loss is 47.84490966796875\n",
      "Current training loss is 47.625789642333984\n",
      "Current training loss is 47.41077423095703\n",
      "Current training loss is 47.19976043701172\n",
      "Current training loss is 46.99270248413086\n",
      "Current training loss is 46.78952407836914\n",
      "Current training loss is 46.590145111083984\n",
      "Current training loss is 46.3945198059082\n",
      "Current training loss is 46.20255661010742\n",
      "Current training loss is 46.014198303222656\n",
      "Current training loss is 45.82939529418945\n",
      "Current training loss is 45.64805603027344\n",
      "Current training loss is 45.470130920410156\n",
      "Current training loss is 45.29555892944336\n",
      "Current training loss is 45.12428665161133\n",
      "Current training loss is 44.95624542236328\n",
      "Current training loss is 44.79138946533203\n",
      "Current training loss is 44.629638671875\n",
      "Current training loss is 44.470951080322266\n",
      "Current training loss is 44.31528091430664\n",
      "Current training loss is 44.162559509277344\n",
      "Current training loss is 44.012733459472656\n",
      "Current training loss is 43.86574935913086\n",
      "Current training loss is 43.721561431884766\n",
      "Current training loss is 43.58012771606445\n",
      "Current training loss is 43.441375732421875\n",
      "Current training loss is 43.30527114868164\n",
      "Current training loss is 43.17176055908203\n",
      "Current training loss is 43.04081344604492\n",
      "Current training loss is 42.91236114501953\n",
      "Current training loss is 42.78636169433594\n",
      "Current training loss is 42.662776947021484\n",
      "Current training loss is 42.54157257080078\n",
      "Current training loss is 42.42267990112305\n",
      "Current training loss is 42.306068420410156\n",
      "Current training loss is 42.19170379638672\n",
      "Current training loss is 42.07952880859375\n",
      "Current training loss is 41.96952438354492\n",
      "Current training loss is 41.861629486083984\n",
      "Current training loss is 41.75581359863281\n",
      "Current training loss is 41.652034759521484\n",
      "Current training loss is 41.550262451171875\n",
      "Current training loss is 41.45046615600586\n",
      "Current training loss is 41.35258865356445\n",
      "Current training loss is 41.25660705566406\n",
      "Current training loss is 41.1624755859375\n",
      "Current training loss is 41.07018280029297\n",
      "Current training loss is 40.979671478271484\n",
      "Current training loss is 40.89090347290039\n",
      "Current training loss is 40.80387496948242\n",
      "Current training loss is 40.718528747558594\n",
      "Current training loss is 40.634849548339844\n",
      "Current training loss is 40.552791595458984\n",
      "Current training loss is 40.47232437133789\n",
      "Current training loss is 40.39342498779297\n",
      "Current training loss is 40.31606674194336\n",
      "Current training loss is 40.24021911621094\n",
      "Current training loss is 40.16584014892578\n",
      "Current training loss is 40.092918395996094\n",
      "Current training loss is 40.02142333984375\n",
      "Current training loss is 39.95131301879883\n",
      "Current training loss is 39.8825798034668\n",
      "Current training loss is 39.815181732177734\n",
      "Current training loss is 39.749107360839844\n",
      "Current training loss is 39.684326171875\n",
      "Current training loss is 39.62080383300781\n",
      "Current training loss is 39.55853271484375\n",
      "Current training loss is 39.49747848510742\n",
      "Current training loss is 39.4376106262207\n",
      "Current training loss is 39.37892150878906\n",
      "Current training loss is 39.32137680053711\n",
      "Current training loss is 39.26496124267578\n",
      "Current training loss is 39.20964431762695\n",
      "Current training loss is 39.15542221069336\n",
      "Current training loss is 39.10224914550781\n",
      "Current training loss is 39.05011749267578\n",
      "Current training loss is 38.99901580810547\n",
      "Current training loss is 38.94890594482422\n",
      "Current training loss is 38.8997802734375\n",
      "Current training loss is 38.85161590576172\n",
      "Current training loss is 38.80439376831055\n",
      "Current training loss is 38.75809097290039\n",
      "Current training loss is 38.71269607543945\n",
      "Current training loss is 38.66818618774414\n",
      "Current training loss is 38.62453842163086\n",
      "Current training loss is 38.58175277709961\n",
      "Current training loss is 38.539794921875\n",
      "Current training loss is 38.49866485595703\n",
      "Current training loss is 38.45832824707031\n",
      "Current training loss is 38.41878128051758\n",
      "Current training loss is 38.37999725341797\n",
      "Current training loss is 38.341976165771484\n",
      "Current training loss is 38.304683685302734\n",
      "Current training loss is 38.268123626708984\n",
      "Current training loss is 38.23226547241211\n",
      "Current training loss is 38.197105407714844\n",
      "Current training loss is 38.16262435913086\n",
      "Current training loss is 38.128807067871094\n",
      "Current training loss is 38.095645904541016\n",
      "Current training loss is 38.06311798095703\n",
      "Current training loss is 38.03122329711914\n",
      "Current training loss is 37.999942779541016\n",
      "Current training loss is 37.969261169433594\n",
      "Current training loss is 37.93916320800781\n",
      "Current training loss is 37.90964889526367\n",
      "Current training loss is 37.88069152832031\n",
      "Current training loss is 37.85228729248047\n",
      "Current training loss is 37.82442855834961\n",
      "Current training loss is 37.797088623046875\n",
      "Current training loss is 37.770263671875\n",
      "Current training loss is 37.74396514892578\n",
      "Current training loss is 37.71814727783203\n",
      "Current training loss is 37.69281768798828\n",
      "Current training loss is 37.66796875\n",
      "Current training loss is 37.643585205078125\n",
      "Current training loss is 37.619659423828125\n",
      "Current training loss is 37.59617233276367\n",
      "Current training loss is 37.57312774658203\n",
      "Current training loss is 37.55051040649414\n",
      "Current training loss is 37.5283088684082\n",
      "Current training loss is 37.50651168823242\n",
      "Current training loss is 37.48512268066406\n",
      "Current training loss is 37.4641227722168\n",
      "Current training loss is 37.44350814819336\n",
      "Current training loss is 37.42326736450195\n",
      "Current training loss is 37.40339279174805\n",
      "Current training loss is 37.383872985839844\n",
      "Current training loss is 37.36471176147461\n",
      "Current training loss is 37.345890045166016\n",
      "Current training loss is 37.32740783691406\n",
      "Current training loss is 37.30924606323242\n",
      "Current training loss is 37.291412353515625\n",
      "Current training loss is 37.27389144897461\n",
      "Current training loss is 37.25667953491211\n",
      "Current training loss is 37.239768981933594\n",
      "Current training loss is 37.22315216064453\n",
      "Current training loss is 37.206817626953125\n",
      "Current training loss is 37.19076919555664\n",
      "Current training loss is 37.17499542236328\n",
      "Current training loss is 37.15948486328125\n",
      "Current training loss is 37.14424514770508\n",
      "Current training loss is 37.1292610168457\n",
      "Current training loss is 37.11452865600586\n",
      "Current training loss is 37.10003662109375\n",
      "Current training loss is 37.08578872680664\n",
      "Current training loss is 37.07176971435547\n",
      "Current training loss is 37.0579833984375\n",
      "Current training loss is 37.04442596435547\n",
      "Current training loss is 37.03108596801758\n",
      "Current training loss is 37.01795196533203\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current training loss is 37.005035400390625\n",
      "Current training loss is 36.99231719970703\n",
      "Current training loss is 36.97980499267578\n",
      "Current training loss is 36.96748352050781\n",
      "Current training loss is 36.955352783203125\n",
      "Current training loss is 36.94340896606445\n",
      "Current training loss is 36.93164825439453\n",
      "Current training loss is 36.92006301879883\n",
      "Current training loss is 36.908653259277344\n",
      "Current training loss is 36.89740753173828\n",
      "Current training loss is 36.8863410949707\n",
      "Current training loss is 36.87541580200195\n",
      "Current training loss is 36.86467361450195\n",
      "Current training loss is 36.854061126708984\n",
      "Current training loss is 36.84361267089844\n",
      "Current training loss is 36.833309173583984\n",
      "Current training loss is 36.82315444946289\n",
      "Current training loss is 36.813133239746094\n",
      "Current training loss is 36.80324935913086\n",
      "Current training loss is 36.79349899291992\n",
      "Current training loss is 36.78387451171875\n",
      "Current training loss is 36.774383544921875\n",
      "Current training loss is 36.76502227783203\n",
      "Current training loss is 36.75577163696289\n",
      "Current training loss is 36.746646881103516\n",
      "Current training loss is 36.737632751464844\n",
      "Current training loss is 36.72874069213867\n",
      "Current training loss is 36.719947814941406\n",
      "Current training loss is 36.71126937866211\n",
      "Current training loss is 36.702693939208984\n",
      "Current training loss is 36.6942138671875\n",
      "Current training loss is 36.68584060668945\n",
      "Current training loss is 36.67756271362305\n",
      "Current training loss is 36.66938018798828\n",
      "Current training loss is 36.661293029785156\n",
      "Current training loss is 36.65329360961914\n",
      "Current training loss is 36.645381927490234\n",
      "Current training loss is 36.63755416870117\n",
      "Current training loss is 36.62981033325195\n",
      "Current training loss is 36.622154235839844\n",
      "Current training loss is 36.61457061767578\n",
      "Current training loss is 36.60707092285156\n",
      "Current training loss is 36.59964370727539\n",
      "Current training loss is 36.59228515625\n",
      "Current training loss is 36.58500671386719\n",
      "Current training loss is 36.577796936035156\n",
      "Current training loss is 36.57065200805664\n",
      "Current training loss is 36.56357955932617\n",
      "Current training loss is 36.55656433105469\n",
      "Current training loss is 36.549617767333984\n",
      "Current training loss is 36.5427360534668\n",
      "Current training loss is 36.53590393066406\n",
      "Current training loss is 36.529136657714844\n",
      "Current training loss is 36.52242660522461\n",
      "Current training loss is 36.515769958496094\n",
      "Current training loss is 36.50917053222656\n",
      "Current training loss is 36.50262451171875\n",
      "Current training loss is 36.49612808227539\n",
      "Current training loss is 36.489681243896484\n",
      "Current training loss is 36.4832878112793\n",
      "Current training loss is 36.4769287109375\n",
      "Current training loss is 36.47063064575195\n",
      "Current training loss is 36.46437072753906\n",
      "Current training loss is 36.45814895629883\n",
      "Current training loss is 36.45198059082031\n",
      "Current training loss is 36.44585037231445\n",
      "Current training loss is 36.439762115478516\n",
      "Current training loss is 36.433712005615234\n",
      "Current training loss is 36.427696228027344\n",
      "Current training loss is 36.42171859741211\n",
      "Current training loss is 36.41578674316406\n",
      "Current training loss is 36.409873962402344\n",
      "Current training loss is 36.40400695800781\n",
      "Current training loss is 36.39816665649414\n",
      "Current training loss is 36.39236831665039\n",
      "Current training loss is 36.386592864990234\n",
      "Current training loss is 36.380855560302734\n",
      "Current training loss is 36.37513732910156\n",
      "Current training loss is 36.36945724487305\n",
      "Current training loss is 36.36379623413086\n",
      "Current training loss is 36.3581657409668\n",
      "Current training loss is 36.35257339477539\n",
      "Current training loss is 36.34699249267578\n",
      "Current training loss is 36.3414421081543\n",
      "Current training loss is 36.33591842651367\n",
      "Current training loss is 36.33041763305664\n",
      "Current training loss is 36.3249397277832\n",
      "Current training loss is 36.319480895996094\n",
      "Current training loss is 36.31404495239258\n",
      "Current training loss is 36.30863571166992\n",
      "Current training loss is 36.30323791503906\n",
      "Current training loss is 36.29786682128906\n",
      "Current training loss is 36.29251480102539\n",
      "Current training loss is 36.28717803955078\n",
      "Current training loss is 36.281856536865234\n",
      "Current training loss is 36.27656173706055\n",
      "Current training loss is 36.27127456665039\n",
      "Current training loss is 36.26601791381836\n",
      "Current training loss is 36.260765075683594\n",
      "Current training loss is 36.25553512573242\n",
      "Current training loss is 36.25031280517578\n",
      "Current training loss is 36.245113372802734\n",
      "Current training loss is 36.23991775512695\n",
      "Current training loss is 36.2347412109375\n",
      "Current training loss is 36.229583740234375\n",
      "Current training loss is 36.22443389892578\n",
      "Current training loss is 36.219295501708984\n",
      "Current training loss is 36.21417236328125\n",
      "Current training loss is 36.20906066894531\n",
      "Current training loss is 36.203956604003906\n",
      "Current training loss is 36.1988639831543\n",
      "Current training loss is 36.19378662109375\n",
      "Current training loss is 36.188716888427734\n",
      "Current training loss is 36.18366241455078\n",
      "Current training loss is 36.178611755371094\n",
      "Current training loss is 36.1735725402832\n",
      "Current training loss is 36.16853713989258\n",
      "Current training loss is 36.163516998291016\n",
      "Current training loss is 36.158504486083984\n",
      "Current training loss is 36.15350341796875\n",
      "Current training loss is 36.14849853515625\n",
      "Current training loss is 36.14351272583008\n",
      "Current training loss is 36.138526916503906\n",
      "Current training loss is 36.13355255126953\n",
      "Current training loss is 36.12858200073242\n",
      "Current training loss is 36.12362289428711\n",
      "Current training loss is 36.1186637878418\n",
      "Current training loss is 36.11371612548828\n",
      "Current training loss is 36.10877227783203\n",
      "Current training loss is 36.10383605957031\n",
      "Current training loss is 36.098899841308594\n",
      "Current training loss is 36.093971252441406\n",
      "Current training loss is 36.089054107666016\n",
      "Current training loss is 36.08413314819336\n",
      "Current training loss is 36.079219818115234\n",
      "Current training loss is 36.07430648803711\n",
      "Current training loss is 36.06941223144531\n",
      "Current training loss is 36.064510345458984\n",
      "Current training loss is 36.05960464477539\n",
      "Current training loss is 36.054718017578125\n",
      "Current training loss is 36.049827575683594\n",
      "Current training loss is 36.044944763183594\n",
      "Current training loss is 36.04006576538086\n",
      "Current training loss is 36.03518295288086\n",
      "Current training loss is 36.03030776977539\n",
      "Current training loss is 36.02543258666992\n",
      "Current training loss is 36.02056884765625\n",
      "Current training loss is 36.01570129394531\n",
      "Current training loss is 36.010833740234375\n",
      "Current training loss is 36.0059700012207\n",
      "Current training loss is 36.00111389160156\n",
      "Current training loss is 35.996253967285156\n",
      "Current training loss is 35.99140167236328\n",
      "Current training loss is 35.98653793334961\n",
      "Current training loss is 35.981689453125\n",
      "Current training loss is 35.97684097290039\n",
      "Current training loss is 35.97199249267578\n",
      "Current training loss is 35.96713638305664\n",
      "Current training loss is 35.96229934692383\n",
      "Current training loss is 35.957454681396484\n",
      "Current training loss is 35.95261001586914\n",
      "Current training loss is 35.94776916503906\n",
      "Current training loss is 35.94292449951172\n",
      "Current training loss is 35.938079833984375\n",
      "Current training loss is 35.93324661254883\n",
      "Current training loss is 35.92840576171875\n",
      "Current training loss is 35.92356491088867\n",
      "Current training loss is 35.91872787475586\n",
      "Current training loss is 35.91389465332031\n",
      "Current training loss is 35.909053802490234\n",
      "Current training loss is 35.90422439575195\n",
      "Current training loss is 35.89938735961914\n",
      "Current training loss is 35.89455032348633\n",
      "Current training loss is 35.88971710205078\n",
      "Current training loss is 35.88488006591797\n",
      "Current training loss is 35.88005065917969\n",
      "Current training loss is 35.87520980834961\n",
      "Current training loss is 35.8703727722168\n",
      "Current training loss is 35.865543365478516\n",
      "Current training loss is 35.8607063293457\n",
      "Current training loss is 35.855873107910156\n",
      "Current training loss is 35.851036071777344\n",
      "Current training loss is 35.84620666503906\n",
      "Current training loss is 35.841365814208984\n",
      "Current training loss is 35.836524963378906\n",
      "Current training loss is 35.83169174194336\n",
      "Current training loss is 35.82685852050781\n",
      "Current training loss is 35.822021484375\n",
      "Current training loss is 35.81718444824219\n",
      "Current training loss is 35.81235122680664\n",
      "Current training loss is 35.80751419067383\n",
      "Current training loss is 35.80267333984375\n",
      "Current training loss is 35.79783248901367\n",
      "Current training loss is 35.79300308227539\n",
      "Current training loss is 35.78815460205078\n",
      "Current training loss is 35.7833137512207\n",
      "Current training loss is 35.77846908569336\n",
      "Current training loss is 35.77362823486328\n",
      "Current training loss is 35.7687873840332\n",
      "Current training loss is 35.763938903808594\n",
      "Current training loss is 35.759098052978516\n",
      "Current training loss is 35.754249572753906\n",
      "Current training loss is 35.749412536621094\n",
      "Current training loss is 35.74456024169922\n",
      "Current training loss is 35.739715576171875\n",
      "Current training loss is 35.73486328125\n",
      "Current training loss is 35.73001480102539\n",
      "Current training loss is 35.725162506103516\n",
      "Current training loss is 35.720314025878906\n",
      "Current training loss is 35.715457916259766\n",
      "Current training loss is 35.710609436035156\n",
      "Current training loss is 35.70574951171875\n",
      "Current training loss is 35.70090103149414\n",
      "Current training loss is 35.696041107177734\n",
      "Current training loss is 35.69118881225586\n",
      "Current training loss is 35.68632507324219\n",
      "Current training loss is 35.68146896362305\n",
      "Current training loss is 35.676605224609375\n",
      "Current training loss is 35.67174530029297\n",
      "Current training loss is 35.66688919067383\n",
      "Current training loss is 35.662017822265625\n",
      "Current training loss is 35.65715789794922\n",
      "Current training loss is 35.652286529541016\n",
      "Current training loss is 35.647422790527344\n",
      "Current training loss is 35.642555236816406\n",
      "Current training loss is 35.6376838684082\n",
      "Current training loss is 35.632816314697266\n",
      "Current training loss is 35.62794494628906\n",
      "Current training loss is 35.623069763183594\n",
      "Current training loss is 35.61819076538086\n",
      "Current training loss is 35.61332321166992\n",
      "Current training loss is 35.60844421386719\n",
      "Current training loss is 35.603572845458984\n",
      "Current training loss is 35.598690032958984\n",
      "Current training loss is 35.59381103515625\n",
      "Current training loss is 35.588932037353516\n",
      "Current training loss is 35.58404541015625\n",
      "Current training loss is 35.579158782958984\n",
      "Current training loss is 35.57427978515625\n",
      "Current training loss is 35.56939697265625\n",
      "Current training loss is 35.56450653076172\n",
      "Current training loss is 35.55961990356445\n",
      "Current training loss is 35.55473327636719\n",
      "Current training loss is 35.549842834472656\n",
      "Current training loss is 35.54494857788086\n",
      "Current training loss is 35.54006576538086\n",
      "Current training loss is 35.53516387939453\n",
      "Current training loss is 35.53026580810547\n",
      "Current training loss is 35.5253791809082\n",
      "Current training loss is 35.52048110961914\n",
      "Current training loss is 35.51557922363281\n",
      "Current training loss is 35.510677337646484\n",
      "Current training loss is 35.505775451660156\n",
      "Current training loss is 35.500877380371094\n",
      "Current training loss is 35.4959716796875\n",
      "Current training loss is 35.49106979370117\n",
      "Current training loss is 35.48616027832031\n",
      "Current training loss is 35.48125076293945\n",
      "Current training loss is 35.476348876953125\n",
      "Current training loss is 35.471439361572266\n",
      "Current training loss is 35.466529846191406\n",
      "Current training loss is 35.46162033081055\n",
      "Current training loss is 35.456703186035156\n",
      "Current training loss is 35.45178985595703\n",
      "Current training loss is 35.446868896484375\n",
      "Current training loss is 35.44195556640625\n",
      "Current training loss is 35.43703842163086\n",
      "Current training loss is 35.43212127685547\n",
      "Current training loss is 35.42720413208008\n",
      "Current training loss is 35.42228317260742\n",
      "Current training loss is 35.4173583984375\n",
      "Current training loss is 35.412437438964844\n",
      "Current training loss is 35.407508850097656\n",
      "Current training loss is 35.402584075927734\n",
      "Current training loss is 35.39765930175781\n",
      "Current training loss is 35.39273452758789\n",
      "Current training loss is 35.38779830932617\n",
      "Current training loss is 35.38287353515625\n",
      "Current training loss is 35.3779411315918\n",
      "Current training loss is 35.37301254272461\n",
      "Current training loss is 35.36807632446289\n",
      "Current training loss is 35.3631477355957\n",
      "Current training loss is 35.35820770263672\n",
      "Current training loss is 35.353267669677734\n",
      "Current training loss is 35.348331451416016\n",
      "Current training loss is 35.3433952331543\n",
      "Current training loss is 35.33845520019531\n",
      "Current training loss is 35.33351135253906\n",
      "Current training loss is 35.328575134277344\n",
      "Current training loss is 35.323631286621094\n",
      "Current training loss is 35.31867980957031\n",
      "Current training loss is 35.31373596191406\n",
      "Current training loss is 35.30879592895508\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current training loss is 35.30384826660156\n",
      "Current training loss is 35.29889678955078\n",
      "Current training loss is 35.293941497802734\n",
      "Current training loss is 35.288997650146484\n",
      "Current training loss is 35.2840461730957\n",
      "Current training loss is 35.27908706665039\n",
      "Current training loss is 35.274139404296875\n",
      "Current training loss is 35.26918411254883\n",
      "Current training loss is 35.26423263549805\n",
      "Current training loss is 35.259273529052734\n",
      "Current training loss is 35.25431823730469\n",
      "Current training loss is 35.24935531616211\n",
      "Current training loss is 35.2443962097168\n",
      "Current training loss is 35.239437103271484\n",
      "Current training loss is 35.23447799682617\n",
      "Current training loss is 35.229515075683594\n",
      "Current training loss is 35.22455596923828\n",
      "Current training loss is 35.21958541870117\n",
      "Current training loss is 35.21462631225586\n",
      "Current training loss is 35.209651947021484\n",
      "Current training loss is 35.204689025878906\n",
      "Current training loss is 35.1997184753418\n",
      "Current training loss is 35.19475173950195\n",
      "Current training loss is 35.189781188964844\n",
      "Current training loss is 35.184810638427734\n",
      "Current training loss is 35.17983627319336\n",
      "Current training loss is 35.17486572265625\n",
      "Current training loss is 35.169891357421875\n",
      "Current training loss is 35.1649169921875\n",
      "Current training loss is 35.15993881225586\n",
      "Current training loss is 35.15496063232422\n",
      "Current training loss is 35.149986267089844\n",
      "Current training loss is 35.1450080871582\n",
      "Current training loss is 35.14003372192383\n",
      "Current training loss is 35.13504409790039\n",
      "Current training loss is 35.13007354736328\n",
      "Current training loss is 35.12509536743164\n",
      "Current training loss is 35.12010955810547\n",
      "Current training loss is 35.11512756347656\n",
      "Current training loss is 35.11013412475586\n",
      "Current training loss is 35.10514831542969\n",
      "Current training loss is 35.10016632080078\n",
      "Current training loss is 35.09518051147461\n",
      "Current training loss is 35.0901985168457\n",
      "Current training loss is 35.085205078125\n",
      "Current training loss is 35.08021926879883\n",
      "Current training loss is 35.07522964477539\n",
      "Current training loss is 35.07023239135742\n",
      "Current training loss is 35.06524658203125\n",
      "Current training loss is 35.06025695800781\n",
      "Current training loss is 35.05526351928711\n",
      "Current training loss is 35.050270080566406\n",
      "Current training loss is 35.0452766418457\n",
      "Current training loss is 35.040279388427734\n",
      "Current training loss is 35.03528594970703\n",
      "Current training loss is 35.03028869628906\n",
      "Current training loss is 35.025291442871094\n",
      "Current training loss is 35.020294189453125\n",
      "Current training loss is 35.015296936035156\n",
      "Current training loss is 35.01029586791992\n",
      "Current training loss is 35.00529479980469\n",
      "Current training loss is 35.00029373168945\n",
      "Current training loss is 34.995296478271484\n",
      "Current training loss is 34.99029541015625\n",
      "Current training loss is 34.985294342041016\n",
      "Current training loss is 34.980289459228516\n",
      "Current training loss is 34.97528076171875\n",
      "Current training loss is 34.97027587890625\n",
      "Current training loss is 34.965274810791016\n",
      "Current training loss is 34.96026611328125\n",
      "Current training loss is 34.95526123046875\n",
      "Current training loss is 34.95025634765625\n",
      "Current training loss is 34.945247650146484\n",
      "Current training loss is 34.94023513793945\n",
      "Current training loss is 34.93522644042969\n",
      "Current training loss is 34.930213928222656\n",
      "Current training loss is 34.92520523071289\n",
      "Current training loss is 34.92020034790039\n",
      "Current training loss is 34.915184020996094\n",
      "Current training loss is 34.910179138183594\n",
      "Current training loss is 34.90515899658203\n",
      "Current training loss is 34.900146484375\n",
      "Current training loss is 34.8951301574707\n",
      "Current training loss is 34.8901252746582\n",
      "Current training loss is 34.88510513305664\n",
      "Current training loss is 34.880088806152344\n",
      "Current training loss is 34.87507629394531\n",
      "Current training loss is 34.87006378173828\n",
      "Current training loss is 34.86503982543945\n",
      "Current training loss is 34.86001968383789\n",
      "Current training loss is 34.85500717163086\n",
      "Current training loss is 34.8499870300293\n",
      "Current training loss is 34.844966888427734\n",
      "Current training loss is 34.83994674682617\n",
      "Current training loss is 34.834922790527344\n",
      "Current training loss is 34.82991027832031\n",
      "Current training loss is 34.824886322021484\n",
      "Current training loss is 34.81986618041992\n",
      "Current training loss is 34.814842224121094\n",
      "Current training loss is 34.809814453125\n",
      "Current training loss is 34.80479431152344\n",
      "Current training loss is 34.79977035522461\n",
      "Current training loss is 34.79475021362305\n",
      "Current training loss is 34.78972244262695\n",
      "Current training loss is 34.78470230102539\n",
      "Current training loss is 34.77967834472656\n",
      "Current training loss is 34.774654388427734\n",
      "Current training loss is 34.769622802734375\n",
      "Current training loss is 34.76459503173828\n",
      "Current training loss is 34.75957107543945\n",
      "Current training loss is 34.754547119140625\n",
      "Current training loss is 34.749515533447266\n",
      "Current training loss is 34.74448776245117\n",
      "Current training loss is 34.73945999145508\n",
      "Current training loss is 34.734432220458984\n",
      "Current training loss is 34.729400634765625\n",
      "Current training loss is 34.72437286376953\n",
      "Current training loss is 34.7193489074707\n",
      "Current training loss is 34.714317321777344\n",
      "Current training loss is 34.709285736083984\n",
      "Current training loss is 34.70425033569336\n",
      "Current training loss is 34.69921875\n",
      "Current training loss is 34.694190979003906\n",
      "Current training loss is 34.68915557861328\n",
      "Current training loss is 34.68412399291992\n",
      "Current training loss is 34.679100036621094\n",
      "Current training loss is 34.6740608215332\n",
      "Current training loss is 34.66902160644531\n",
      "Current training loss is 34.663997650146484\n",
      "Current training loss is 34.65896224975586\n",
      "Current training loss is 34.6539306640625\n",
      "Current training loss is 34.64889144897461\n",
      "Current training loss is 34.64386749267578\n",
      "Current training loss is 34.638824462890625\n",
      "Current training loss is 34.6337890625\n",
      "Current training loss is 34.62874984741211\n",
      "Current training loss is 34.62371826171875\n",
      "Current training loss is 34.618682861328125\n",
      "Current training loss is 34.613643646240234\n",
      "Current training loss is 34.608612060546875\n",
      "Current training loss is 34.603580474853516\n",
      "Current training loss is 34.59854507446289\n",
      "Current training loss is 34.593502044677734\n",
      "Current training loss is 34.588462829589844\n",
      "Current training loss is 34.58342742919922\n",
      "Current training loss is 34.578392028808594\n",
      "Current training loss is 34.57335662841797\n",
      "Current training loss is 34.56831359863281\n",
      "Current training loss is 34.56327438354492\n",
      "Current training loss is 34.5582389831543\n",
      "Current training loss is 34.553199768066406\n",
      "Current training loss is 34.54816436767578\n",
      "Current training loss is 34.54312515258789\n",
      "Current training loss is 34.538089752197266\n",
      "Current training loss is 34.53304672241211\n",
      "Current training loss is 34.528011322021484\n",
      "Current training loss is 34.52296447753906\n",
      "Current training loss is 34.51792907714844\n",
      "Current training loss is 34.51289749145508\n",
      "Current training loss is 34.50785446166992\n",
      "Current training loss is 34.50281524658203\n",
      "Current training loss is 34.49777603149414\n",
      "Current training loss is 34.492733001708984\n",
      "Current training loss is 34.48769760131836\n",
      "Current training loss is 34.4826545715332\n",
      "Current training loss is 34.47761535644531\n",
      "Current training loss is 34.47257614135742\n",
      "Current training loss is 34.46753692626953\n",
      "Current training loss is 34.462493896484375\n",
      "Current training loss is 34.457454681396484\n",
      "Current training loss is 34.452415466308594\n",
      "Current training loss is 34.44737243652344\n",
      "Current training loss is 34.44233322143555\n",
      "Current training loss is 34.437294006347656\n",
      "Current training loss is 34.432254791259766\n",
      "Current training loss is 34.427215576171875\n",
      "Current training loss is 34.422176361083984\n",
      "Current training loss is 34.417137145996094\n",
      "Current training loss is 34.41209030151367\n",
      "Current training loss is 34.40705871582031\n",
      "Current training loss is 34.402015686035156\n",
      "Current training loss is 34.39697265625\n",
      "Current training loss is 34.391929626464844\n",
      "Current training loss is 34.38689041137695\n",
      "Current training loss is 34.38185119628906\n",
      "Current training loss is 34.37680435180664\n",
      "Current training loss is 34.37177658081055\n",
      "Current training loss is 34.36673355102539\n",
      "Current training loss is 34.361690521240234\n",
      "Current training loss is 34.356651306152344\n",
      "Current training loss is 34.35161590576172\n",
      "Current training loss is 34.34657287597656\n",
      "Current training loss is 34.34153366088867\n",
      "Current training loss is 34.33649444580078\n",
      "Current training loss is 34.33146286010742\n",
      "Current training loss is 34.326416015625\n",
      "Current training loss is 34.32137680053711\n",
      "Current training loss is 34.316341400146484\n",
      "Current training loss is 34.31129837036133\n",
      "Current training loss is 34.3062629699707\n",
      "Current training loss is 34.30121994018555\n",
      "Current training loss is 34.296180725097656\n",
      "Current training loss is 34.29114532470703\n",
      "Current training loss is 34.28610610961914\n",
      "Current training loss is 34.28106689453125\n",
      "Current training loss is 34.27603530883789\n",
      "Current training loss is 34.270992279052734\n",
      "Current training loss is 34.26595687866211\n",
      "Current training loss is 34.26091384887695\n",
      "Current training loss is 34.25587844848633\n",
      "Current training loss is 34.25084686279297\n",
      "Current training loss is 34.24580383300781\n",
      "Current training loss is 34.24077224731445\n",
      "Current training loss is 34.2357292175293\n",
      "Current training loss is 34.23069763183594\n",
      "Current training loss is 34.22566223144531\n",
      "Current training loss is 34.22062301635742\n",
      "Current training loss is 34.21559143066406\n",
      "Current training loss is 34.21055221557617\n",
      "Current training loss is 34.20551300048828\n",
      "Current training loss is 34.20048141479492\n",
      "Current training loss is 34.19544982910156\n",
      "Current training loss is 34.1904182434082\n",
      "Current training loss is 34.18537521362305\n",
      "Current training loss is 34.18033981323242\n",
      "Current training loss is 34.17530822753906\n",
      "Current training loss is 34.1702766418457\n",
      "Current training loss is 34.165245056152344\n",
      "Current training loss is 34.16020965576172\n",
      "Current training loss is 34.155174255371094\n",
      "Current training loss is 34.150142669677734\n",
      "Current training loss is 34.145111083984375\n",
      "Current training loss is 34.14008331298828\n",
      "Current training loss is 34.135047912597656\n",
      "Current training loss is 34.1300163269043\n",
      "Current training loss is 34.1249885559082\n",
      "Current training loss is 34.119956970214844\n",
      "Current training loss is 34.114925384521484\n",
      "Current training loss is 34.10989761352539\n",
      "Current training loss is 34.10486602783203\n",
      "Current training loss is 34.09983825683594\n",
      "Current training loss is 34.09481430053711\n",
      "Current training loss is 34.08977508544922\n",
      "Current training loss is 34.08475112915039\n",
      "Current training loss is 34.07972717285156\n",
      "Current training loss is 34.0746955871582\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current training loss is 34.06966781616211\n",
      "Current training loss is 34.06464385986328\n",
      "Current training loss is 34.05961608886719\n",
      "Current training loss is 34.054588317871094\n",
      "Current training loss is 34.049564361572266\n",
      "Current training loss is 34.04453659057617\n",
      "Current training loss is 34.03951644897461\n",
      "Current training loss is 34.03449249267578\n",
      "Current training loss is 34.02946472167969\n",
      "Current training loss is 34.02444076538086\n",
      "Current training loss is 34.01941680908203\n",
      "Current training loss is 34.014400482177734\n",
      "Current training loss is 34.00938034057617\n",
      "Current training loss is 34.00435256958008\n",
      "Current training loss is 33.99933624267578\n",
      "Current training loss is 33.99431228637695\n",
      "Current training loss is 33.989295959472656\n",
      "Current training loss is 33.984275817871094\n",
      "Current training loss is 33.979251861572266\n",
      "Current training loss is 33.97423553466797\n",
      "Current training loss is 33.96921157836914\n",
      "Current training loss is 33.964195251464844\n",
      "Current training loss is 33.95918273925781\n",
      "Current training loss is 33.95416259765625\n",
      "Current training loss is 33.94914627075195\n",
      "Current training loss is 33.944129943847656\n",
      "Current training loss is 33.93911361694336\n",
      "Current training loss is 33.93409729003906\n",
      "Current training loss is 33.92908477783203\n",
      "Current training loss is 33.924072265625\n",
      "Current training loss is 33.91905975341797\n",
      "Current training loss is 33.91404342651367\n",
      "Current training loss is 33.90903854370117\n",
      "Current training loss is 33.904022216796875\n",
      "Current training loss is 33.89901351928711\n",
      "Current training loss is 33.89400100708008\n",
      "Current training loss is 33.88899230957031\n",
      "Current training loss is 33.883975982666016\n",
      "Current training loss is 33.87897872924805\n",
      "Current training loss is 33.873966217041016\n",
      "Current training loss is 33.86895751953125\n",
      "Current training loss is 33.863948822021484\n",
      "Current training loss is 33.858943939208984\n",
      "Current training loss is 33.853939056396484\n",
      "Current training loss is 33.84893798828125\n",
      "Current training loss is 33.843929290771484\n",
      "Current training loss is 33.83892059326172\n",
      "Current training loss is 33.83392333984375\n",
      "Current training loss is 33.828914642333984\n",
      "Current training loss is 33.82391357421875\n",
      "Current training loss is 33.818912506103516\n",
      "Current training loss is 33.81391525268555\n",
      "Current training loss is 33.80891418457031\n",
      "Current training loss is 33.80391311645508\n",
      "Current training loss is 33.798912048339844\n",
      "Current training loss is 33.793914794921875\n",
      "Current training loss is 33.788917541503906\n",
      "Current training loss is 33.7839241027832\n",
      "Current training loss is 33.7789306640625\n",
      "Current training loss is 33.773929595947266\n",
      "Current training loss is 33.76893615722656\n",
      "Current training loss is 33.76394271850586\n",
      "Current training loss is 33.758949279785156\n",
      "Current training loss is 33.75395584106445\n",
      "Current training loss is 33.74896240234375\n",
      "Current training loss is 33.74396896362305\n",
      "Current training loss is 33.738983154296875\n",
      "Current training loss is 33.73399353027344\n",
      "Current training loss is 33.72900390625\n",
      "Current training loss is 33.7240104675293\n",
      "Current training loss is 33.71902847290039\n",
      "Current training loss is 33.71403884887695\n",
      "Current training loss is 33.70905303955078\n",
      "Current training loss is 33.70406723022461\n",
      "Current training loss is 33.69908142089844\n",
      "Current training loss is 33.69409942626953\n",
      "Current training loss is 33.68912124633789\n",
      "Current training loss is 33.68412399291992\n",
      "Current training loss is 33.67915344238281\n",
      "Current training loss is 33.67416763305664\n",
      "Current training loss is 33.669189453125\n",
      "Current training loss is 33.66421127319336\n",
      "Current training loss is 33.659236907958984\n",
      "Current training loss is 33.65425109863281\n",
      "Current training loss is 33.6492805480957\n",
      "Current training loss is 33.6442985534668\n",
      "Current training loss is 33.63932418823242\n",
      "Current training loss is 33.63434600830078\n",
      "Current training loss is 33.62937927246094\n",
      "Current training loss is 33.62440490722656\n",
      "Current training loss is 33.61943435668945\n",
      "Current training loss is 33.61445999145508\n",
      "Current training loss is 33.609493255615234\n",
      "Current training loss is 33.60451889038086\n",
      "Current training loss is 33.599552154541016\n",
      "Current training loss is 33.594581604003906\n",
      "Current training loss is 33.58961868286133\n",
      "Current training loss is 33.584651947021484\n",
      "Current training loss is 33.57968521118164\n",
      "Current training loss is 33.57472610473633\n",
      "Current training loss is 33.56976318359375\n",
      "Current training loss is 33.56480026245117\n",
      "Current training loss is 33.559837341308594\n",
      "Current training loss is 33.55487823486328\n",
      "Current training loss is 33.5499153137207\n",
      "Current training loss is 33.54494857788086\n",
      "Current training loss is 33.540000915527344\n",
      "Current training loss is 33.5350456237793\n",
      "Current training loss is 33.530086517333984\n",
      "Current training loss is 33.5251350402832\n",
      "Current training loss is 33.52017593383789\n",
      "Current training loss is 33.51522445678711\n",
      "Current training loss is 33.510276794433594\n",
      "Current training loss is 33.50532150268555\n",
      "Current training loss is 33.5003662109375\n",
      "Current training loss is 33.49542236328125\n",
      "Current training loss is 33.490474700927734\n",
      "Current training loss is 33.48552703857422\n",
      "Current training loss is 33.4805793762207\n",
      "Current training loss is 33.47563934326172\n",
      "Current training loss is 33.4706916809082\n",
      "Current training loss is 33.46574401855469\n",
      "Current training loss is 33.46080780029297\n",
      "Current training loss is 33.45586013793945\n",
      "Current training loss is 33.450923919677734\n",
      "Current training loss is 33.445980072021484\n",
      "Current training loss is 33.44104766845703\n",
      "Current training loss is 33.43610382080078\n",
      "Current training loss is 33.43116760253906\n",
      "Current training loss is 33.426231384277344\n",
      "Current training loss is 33.421295166015625\n",
      "Current training loss is 33.41636657714844\n",
      "Current training loss is 33.411434173583984\n",
      "Current training loss is 33.40650177001953\n",
      "Current training loss is 33.401573181152344\n",
      "Current training loss is 33.39664077758789\n",
      "Current training loss is 33.391719818115234\n",
      "Current training loss is 33.38678741455078\n",
      "Current training loss is 33.381858825683594\n",
      "Current training loss is 33.37693786621094\n",
      "Current training loss is 33.37200927734375\n",
      "Current training loss is 33.367088317871094\n",
      "Current training loss is 33.36216354370117\n",
      "Current training loss is 33.35724639892578\n",
      "Current training loss is 33.352325439453125\n",
      "Current training loss is 33.3474006652832\n",
      "Current training loss is 33.342491149902344\n",
      "Current training loss is 33.33757400512695\n",
      "Current training loss is 33.3326530456543\n",
      "Current training loss is 33.32774353027344\n",
      "Current training loss is 33.32283401489258\n",
      "Current training loss is 33.31791687011719\n",
      "Current training loss is 33.313011169433594\n",
      "Current training loss is 33.308101654052734\n",
      "Current training loss is 33.30318832397461\n",
      "Current training loss is 33.29827880859375\n",
      "Current training loss is 33.29337692260742\n",
      "Current training loss is 33.28846740722656\n",
      "Current training loss is 33.283565521240234\n",
      "Current training loss is 33.27865982055664\n",
      "Current training loss is 33.27376174926758\n",
      "Current training loss is 33.26886749267578\n",
      "Current training loss is 33.26396179199219\n",
      "Current training loss is 33.25905990600586\n",
      "Current training loss is 33.25416946411133\n",
      "Current training loss is 33.249267578125\n",
      "Current training loss is 33.24437713623047\n",
      "Current training loss is 33.2394905090332\n",
      "Current training loss is 33.234596252441406\n",
      "Current training loss is 33.22970199584961\n",
      "Current training loss is 33.22480773925781\n",
      "Current training loss is 33.21992111206055\n",
      "Current training loss is 33.21503448486328\n",
      "Current training loss is 33.210147857666016\n",
      "Current training loss is 33.20526123046875\n",
      "Current training loss is 33.20037841796875\n",
      "Current training loss is 33.19549560546875\n",
      "Current training loss is 33.190616607666016\n",
      "Current training loss is 33.18573760986328\n",
      "Current training loss is 33.18085861206055\n",
      "Current training loss is 33.17598342895508\n",
      "Current training loss is 33.171104431152344\n",
      "Current training loss is 33.166229248046875\n",
      "Current training loss is 33.16135787963867\n",
      "Current training loss is 33.1564826965332\n",
      "Current training loss is 33.151615142822266\n",
      "Current training loss is 33.14674377441406\n",
      "Current training loss is 33.14187240600586\n",
      "Current training loss is 33.13701248168945\n",
      "Current training loss is 33.13214111328125\n",
      "Current training loss is 33.12727737426758\n",
      "Current training loss is 33.12240982055664\n",
      "Current training loss is 33.117549896240234\n",
      "Current training loss is 33.112693786621094\n",
      "Current training loss is 33.10783767700195\n",
      "Current training loss is 33.10297775268555\n",
      "Current training loss is 33.09811782836914\n",
      "Current training loss is 33.093257904052734\n",
      "Current training loss is 33.08840560913086\n",
      "Current training loss is 33.083553314208984\n",
      "Current training loss is 33.07870101928711\n",
      "Current training loss is 33.0738525390625\n",
      "Current training loss is 33.06900405883789\n",
      "Current training loss is 33.06415557861328\n",
      "Current training loss is 33.05931091308594\n",
      "Current training loss is 33.054466247558594\n",
      "Current training loss is 33.04962158203125\n",
      "Current training loss is 33.04478073120117\n",
      "Current training loss is 33.039939880371094\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current training loss is 33.03510284423828\n",
      "Current training loss is 33.03025817871094\n",
      "Current training loss is 33.02542495727539\n",
      "Current training loss is 33.020591735839844\n",
      "Current training loss is 33.0157585144043\n",
      "Current training loss is 33.01092529296875\n",
      "Current training loss is 33.006099700927734\n",
      "Current training loss is 33.00126647949219\n",
      "Current training loss is 32.99643325805664\n",
      "Current training loss is 32.99161148071289\n",
      "Current training loss is 32.98678970336914\n",
      "Current training loss is 32.981964111328125\n",
      "Current training loss is 32.97713851928711\n",
      "Current training loss is 32.972312927246094\n",
      "Current training loss is 32.967498779296875\n",
      "Current training loss is 32.96268081665039\n",
      "Current training loss is 32.95786666870117\n",
      "Current training loss is 32.95304489135742\n",
      "Current training loss is 32.94823455810547\n",
      "Current training loss is 32.94342041015625\n",
      "Current training loss is 32.93861389160156\n",
      "Current training loss is 32.933799743652344\n",
      "Current training loss is 32.928993225097656\n",
      "Current training loss is 32.924190521240234\n",
      "Current training loss is 32.91938018798828\n",
      "Current training loss is 32.91457748413086\n",
      "Current training loss is 32.9097785949707\n",
      "Current training loss is 32.90497589111328\n",
      "Current training loss is 32.900177001953125\n",
      "Current training loss is 32.8953857421875\n",
      "Current training loss is 32.89058303833008\n",
      "Current training loss is 32.88579177856445\n",
      "Current training loss is 32.88099670410156\n",
      "Current training loss is 32.87620162963867\n",
      "Current training loss is 32.87141418457031\n",
      "Current training loss is 32.86662673950195\n",
      "Current training loss is 32.861839294433594\n",
      "Current training loss is 32.8570556640625\n",
      "Current training loss is 32.852264404296875\n",
      "Current training loss is 32.84748458862305\n",
      "Current training loss is 32.842708587646484\n",
      "Current training loss is 32.83792495727539\n",
      "Current training loss is 32.833152770996094\n",
      "Current training loss is 32.82837677001953\n",
      "Current training loss is 32.823604583740234\n",
      "Current training loss is 32.81882858276367\n",
      "Current training loss is 32.81406021118164\n",
      "Current training loss is 32.809288024902344\n",
      "Current training loss is 32.80451965332031\n",
      "Current training loss is 32.79975128173828\n",
      "Current training loss is 32.79499053955078\n",
      "Current training loss is 32.79022216796875\n",
      "Current training loss is 32.785457611083984\n",
      "Current training loss is 32.78070068359375\n",
      "Current training loss is 32.77594757080078\n",
      "Current training loss is 32.77118682861328\n",
      "Current training loss is 32.76643371582031\n",
      "Current training loss is 32.76167297363281\n",
      "Current training loss is 32.75692367553711\n",
      "Current training loss is 32.75217056274414\n",
      "Current training loss is 32.74742126464844\n",
      "Current training loss is 32.74267578125\n",
      "Current training loss is 32.73793411254883\n",
      "Current training loss is 32.733184814453125\n",
      "Current training loss is 32.72844314575195\n",
      "Current training loss is 32.72370529174805\n",
      "Current training loss is 32.71895980834961\n",
      "Current training loss is 32.714229583740234\n",
      "Current training loss is 32.70948791503906\n",
      "Current training loss is 32.70475387573242\n",
      "Current training loss is 32.70002365112305\n",
      "Current training loss is 32.69529342651367\n",
      "Current training loss is 32.69055938720703\n",
      "Current training loss is 32.68583679199219\n",
      "Current training loss is 32.681114196777344\n",
      "Current training loss is 32.676387786865234\n",
      "Current training loss is 32.67166519165039\n",
      "Current training loss is 32.66694259643555\n",
      "Current training loss is 32.6622200012207\n",
      "Current training loss is 32.657501220703125\n",
      "Current training loss is 32.65278625488281\n",
      "Current training loss is 32.6480712890625\n",
      "Current training loss is 32.64336013793945\n",
      "Current training loss is 32.63865280151367\n",
      "Current training loss is 32.633941650390625\n",
      "Current training loss is 32.62923812866211\n",
      "Current training loss is 32.62452697753906\n",
      "Current training loss is 32.61983108520508\n",
      "Current training loss is 32.61511993408203\n",
      "Current training loss is 32.61042785644531\n",
      "Current training loss is 32.60572814941406\n",
      "Current training loss is 32.60102081298828\n",
      "Current training loss is 32.59632873535156\n",
      "Current training loss is 32.59164047241211\n",
      "Current training loss is 32.586952209472656\n",
      "Current training loss is 32.58226013183594\n",
      "Current training loss is 32.577571868896484\n",
      "Current training loss is 32.57288360595703\n",
      "Current training loss is 32.568199157714844\n",
      "Current training loss is 32.56351852416992\n",
      "Current training loss is 32.558837890625\n",
      "Current training loss is 32.55415344238281\n",
      "Current training loss is 32.54947280883789\n",
      "Current training loss is 32.544803619384766\n",
      "Current training loss is 32.54012680053711\n",
      "Current training loss is 32.535457611083984\n",
      "Current training loss is 32.530784606933594\n",
      "Current training loss is 32.52611541748047\n",
      "Current training loss is 32.52145004272461\n",
      "Current training loss is 32.516780853271484\n",
      "Current training loss is 32.51211929321289\n",
      "Current training loss is 32.50746154785156\n",
      "Current training loss is 32.502803802490234\n",
      "Current training loss is 32.498146057128906\n",
      "Current training loss is 32.49348449707031\n",
      "Current training loss is 32.48883056640625\n",
      "Current training loss is 32.48418045043945\n",
      "Current training loss is 32.479530334472656\n",
      "Current training loss is 32.47488021850586\n",
      "Current training loss is 32.470237731933594\n",
      "Current training loss is 32.46559143066406\n",
      "Current training loss is 32.4609489440918\n",
      "Current training loss is 32.45630645751953\n",
      "Current training loss is 32.451663970947266\n",
      "Current training loss is 32.4470329284668\n",
      "Current training loss is 32.44239807128906\n",
      "Current training loss is 32.43776321411133\n",
      "Current training loss is 32.43312454223633\n",
      "Current training loss is 32.42850112915039\n",
      "Current training loss is 32.423866271972656\n",
      "Current training loss is 32.41924285888672\n",
      "Current training loss is 32.41461944580078\n",
      "Current training loss is 32.409996032714844\n",
      "Current training loss is 32.40537643432617\n",
      "Current training loss is 32.400753021240234\n",
      "Current training loss is 32.39613723754883\n",
      "Current training loss is 32.39152526855469\n",
      "Current training loss is 32.38691711425781\n",
      "Current training loss is 32.38230514526367\n",
      "Current training loss is 32.377689361572266\n",
      "Current training loss is 32.37308120727539\n",
      "Current training loss is 32.36847686767578\n",
      "Current training loss is 32.3638801574707\n",
      "Current training loss is 32.359275817871094\n",
      "Current training loss is 32.354671478271484\n",
      "Current training loss is 32.35007858276367\n",
      "Current training loss is 32.345481872558594\n",
      "Current training loss is 32.34088897705078\n",
      "Current training loss is 32.33629608154297\n",
      "Current training loss is 32.33170700073242\n",
      "Current training loss is 32.327117919921875\n",
      "Current training loss is 32.322532653808594\n",
      "Current training loss is 32.31794738769531\n",
      "Current training loss is 32.31336212158203\n",
      "Current training loss is 32.30878448486328\n",
      "Current training loss is 32.30421447753906\n",
      "Current training loss is 32.29963302612305\n",
      "Current training loss is 32.2950553894043\n",
      "Current training loss is 32.29048538208008\n",
      "Current training loss is 32.28591537475586\n",
      "Current training loss is 32.28134536743164\n",
      "Current training loss is 32.27678680419922\n",
      "Current training loss is 32.272216796875\n",
      "Current training loss is 32.26765441894531\n",
      "Current training loss is 32.263092041015625\n",
      "Current training loss is 32.25852966308594\n",
      "Current training loss is 32.25397491455078\n",
      "Current training loss is 32.24942398071289\n",
      "Current training loss is 32.244873046875\n",
      "Current training loss is 32.24032211303711\n",
      "Current training loss is 32.23576736450195\n",
      "Current training loss is 32.23122787475586\n",
      "Current training loss is 32.2266845703125\n",
      "Current training loss is 32.22214126586914\n",
      "Current training loss is 32.21759796142578\n",
      "Current training loss is 32.21306610107422\n",
      "Current training loss is 32.20853042602539\n",
      "Current training loss is 32.20399475097656\n",
      "Current training loss is 32.199459075927734\n",
      "Current training loss is 32.19493103027344\n",
      "Current training loss is 32.19039535522461\n",
      "Current training loss is 32.185874938964844\n",
      "Current training loss is 32.18135070800781\n",
      "Current training loss is 32.17682647705078\n",
      "Current training loss is 32.17230987548828\n",
      "Current training loss is 32.16779327392578\n",
      "Current training loss is 32.16327667236328\n",
      "Current training loss is 32.15876770019531\n",
      "Current training loss is 32.15425109863281\n",
      "Current training loss is 32.149742126464844\n",
      "Current training loss is 32.14523696899414\n",
      "Current training loss is 32.14072799682617\n",
      "Current training loss is 32.136226654052734\n",
      "Current training loss is 32.13172149658203\n",
      "Current training loss is 32.127227783203125\n",
      "Current training loss is 32.12273025512695\n",
      "Current training loss is 32.11823654174805\n",
      "Current training loss is 32.113739013671875\n",
      "Current training loss is 32.1092529296875\n",
      "Current training loss is 32.10476303100586\n",
      "Current training loss is 32.100276947021484\n",
      "Current training loss is 32.095787048339844\n",
      "Current training loss is 32.09130859375\n",
      "Current training loss is 32.08682632446289\n",
      "Current training loss is 32.08235168457031\n",
      "Current training loss is 32.077877044677734\n",
      "Current training loss is 32.07340621948242\n",
      "Current training loss is 32.068931579589844\n",
      "Current training loss is 32.0644645690918\n",
      "Current training loss is 32.05999755859375\n",
      "Current training loss is 32.05552673339844\n",
      "Current training loss is 32.051063537597656\n",
      "Current training loss is 32.046607971191406\n",
      "Current training loss is 32.04214859008789\n",
      "Current training loss is 32.03769302368164\n",
      "Current training loss is 32.033241271972656\n",
      "Current training loss is 32.02878189086914\n",
      "Current training loss is 32.02433395385742\n",
      "Current training loss is 32.0198860168457\n",
      "Current training loss is 32.015438079833984\n",
      "Current training loss is 32.0109977722168\n",
      "Current training loss is 32.00655746459961\n",
      "Current training loss is 32.002113342285156\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current training loss is 31.9976806640625\n",
      "Current training loss is 31.993247985839844\n",
      "Current training loss is 31.98881721496582\n",
      "Current training loss is 31.9843807220459\n",
      "Current training loss is 31.979955673217773\n",
      "Current training loss is 31.975528717041016\n",
      "Current training loss is 31.971107482910156\n",
      "Current training loss is 31.966678619384766\n",
      "Current training loss is 31.96225929260254\n",
      "Current training loss is 31.95784568786621\n",
      "Current training loss is 31.95343017578125\n",
      "Current training loss is 31.949018478393555\n",
      "Current training loss is 31.944608688354492\n",
      "Current training loss is 31.94019889831543\n",
      "Current training loss is 31.935794830322266\n",
      "Current training loss is 31.9313907623291\n",
      "Current training loss is 31.926984786987305\n",
      "Current training loss is 31.922590255737305\n",
      "Current training loss is 31.918188095092773\n",
      "Current training loss is 31.913793563842773\n",
      "Current training loss is 31.90940284729004\n",
      "Current training loss is 31.905014038085938\n",
      "Current training loss is 31.900619506835938\n",
      "Current training loss is 31.896240234375\n",
      "Current training loss is 31.891857147216797\n",
      "Current training loss is 31.887470245361328\n",
      "Current training loss is 31.883094787597656\n",
      "Current training loss is 31.878713607788086\n",
      "Current training loss is 31.874340057373047\n",
      "Current training loss is 31.869970321655273\n",
      "Current training loss is 31.8655948638916\n",
      "Current training loss is 31.861228942871094\n",
      "Current training loss is 31.856863021850586\n",
      "Current training loss is 31.85250473022461\n",
      "Current training loss is 31.84814453125\n",
      "Current training loss is 31.843780517578125\n",
      "Current training loss is 31.839426040649414\n",
      "Current training loss is 31.835073471069336\n",
      "Current training loss is 31.830718994140625\n",
      "Current training loss is 31.82636833190918\n",
      "Current training loss is 31.8220157623291\n",
      "Current training loss is 31.817678451538086\n",
      "Current training loss is 31.813337326049805\n",
      "Current training loss is 31.808990478515625\n",
      "Current training loss is 31.804655075073242\n",
      "Current training loss is 31.80031967163086\n",
      "Current training loss is 31.79598617553711\n",
      "Current training loss is 31.79165267944336\n",
      "Current training loss is 31.787322998046875\n",
      "Current training loss is 31.78299331665039\n",
      "Current training loss is 31.77867317199707\n",
      "Current training loss is 31.77435302734375\n",
      "Current training loss is 31.7700252532959\n",
      "Current training loss is 31.765716552734375\n",
      "Current training loss is 31.761398315429688\n",
      "Current training loss is 31.757081985473633\n",
      "Current training loss is 31.75277328491211\n",
      "Current training loss is 31.748470306396484\n",
      "Current training loss is 31.74416160583496\n",
      "Current training loss is 31.73986053466797\n",
      "Current training loss is 31.73555564880371\n",
      "Current training loss is 31.73125648498535\n",
      "Current training loss is 31.72696304321289\n",
      "Current training loss is 31.72266960144043\n",
      "Current training loss is 31.7183780670166\n",
      "Current training loss is 31.71408462524414\n",
      "Current training loss is 31.709800720214844\n",
      "Current training loss is 31.705516815185547\n",
      "Current training loss is 31.701234817504883\n",
      "Current training loss is 31.696956634521484\n",
      "Current training loss is 31.692678451538086\n",
      "Current training loss is 31.68840217590332\n",
      "Current training loss is 31.68413543701172\n",
      "Current training loss is 31.67985725402832\n",
      "Current training loss is 31.67559051513672\n",
      "Current training loss is 31.671327590942383\n",
      "Current training loss is 31.667064666748047\n",
      "Current training loss is 31.66280174255371\n",
      "Current training loss is 31.658546447753906\n",
      "Current training loss is 31.65428924560547\n",
      "Current training loss is 31.6500301361084\n",
      "Current training loss is 31.645782470703125\n",
      "Current training loss is 31.64153289794922\n",
      "Current training loss is 31.63728904724121\n",
      "Current training loss is 31.63304901123047\n",
      "Current training loss is 31.62881088256836\n",
      "Current training loss is 31.62456512451172\n",
      "Current training loss is 31.62032699584961\n",
      "Current training loss is 31.616090774536133\n",
      "Current training loss is 31.61186408996582\n",
      "Current training loss is 31.60763168334961\n",
      "Current training loss is 31.603404998779297\n",
      "Current training loss is 31.59917640686035\n",
      "Current training loss is 31.594955444335938\n",
      "Current training loss is 31.590736389160156\n",
      "Current training loss is 31.586517333984375\n",
      "Current training loss is 31.582305908203125\n",
      "Current training loss is 31.578094482421875\n",
      "Current training loss is 31.573883056640625\n",
      "Current training loss is 31.569677352905273\n",
      "Current training loss is 31.56546974182129\n",
      "Current training loss is 31.5612735748291\n",
      "Current training loss is 31.55707359313965\n",
      "Current training loss is 31.55286979675293\n",
      "Current training loss is 31.548675537109375\n",
      "Current training loss is 31.54448127746582\n",
      "Current training loss is 31.540294647216797\n",
      "Current training loss is 31.536100387573242\n",
      "Current training loss is 31.531925201416016\n",
      "Current training loss is 31.527738571166992\n",
      "Current training loss is 31.523555755615234\n",
      "Current training loss is 31.519371032714844\n",
      "Current training loss is 31.515199661254883\n",
      "Current training loss is 31.511022567749023\n",
      "Current training loss is 31.506855010986328\n",
      "Current training loss is 31.5026912689209\n",
      "Current training loss is 31.498525619506836\n",
      "Current training loss is 31.49436378479004\n",
      "Current training loss is 31.490203857421875\n",
      "Current training loss is 31.486040115356445\n",
      "Current training loss is 31.481884002685547\n",
      "Current training loss is 31.47773551940918\n",
      "Current training loss is 31.473583221435547\n",
      "Current training loss is 31.469432830810547\n",
      "Current training loss is 31.465290069580078\n",
      "Current training loss is 31.461143493652344\n",
      "Current training loss is 31.45700454711914\n",
      "Current training loss is 31.452871322631836\n",
      "Current training loss is 31.448734283447266\n",
      "Current training loss is 31.444602966308594\n",
      "Current training loss is 31.44046401977539\n",
      "Current training loss is 31.43634033203125\n",
      "Current training loss is 31.43221664428711\n",
      "Current training loss is 31.42808723449707\n",
      "Current training loss is 31.423969268798828\n",
      "Current training loss is 31.41985321044922\n",
      "Current training loss is 31.415740966796875\n",
      "Current training loss is 31.411623001098633\n",
      "Current training loss is 31.407516479492188\n",
      "Current training loss is 31.40340232849121\n",
      "Current training loss is 31.399301528930664\n",
      "Current training loss is 31.39519500732422\n",
      "Current training loss is 31.391103744506836\n",
      "Current training loss is 31.386999130249023\n",
      "Current training loss is 31.38290786743164\n",
      "Current training loss is 31.378814697265625\n",
      "Current training loss is 31.374723434448242\n",
      "Current training loss is 31.37063217163086\n",
      "Current training loss is 31.366552352905273\n",
      "Current training loss is 31.362464904785156\n",
      "Current training loss is 31.358388900756836\n",
      "Current training loss is 31.354310989379883\n",
      "Current training loss is 31.35023307800293\n",
      "Current training loss is 31.346160888671875\n",
      "Current training loss is 31.34209442138672\n",
      "Current training loss is 31.338031768798828\n",
      "Current training loss is 31.33396339416504\n",
      "Current training loss is 31.329904556274414\n",
      "Current training loss is 31.32583999633789\n",
      "Current training loss is 31.321781158447266\n",
      "Current training loss is 31.31772804260254\n",
      "Current training loss is 31.313682556152344\n",
      "Current training loss is 31.30963134765625\n",
      "Current training loss is 31.305587768554688\n",
      "Current training loss is 31.301542282104492\n",
      "Current training loss is 31.297496795654297\n",
      "Current training loss is 31.29345703125\n",
      "Current training loss is 31.2894287109375\n",
      "Current training loss is 31.28539276123047\n",
      "Current training loss is 31.28135871887207\n",
      "Current training loss is 31.277334213256836\n",
      "Current training loss is 31.27330780029297\n",
      "Current training loss is 31.2692813873291\n",
      "Current training loss is 31.265262603759766\n",
      "Current training loss is 31.26124382019043\n",
      "Current training loss is 31.25722885131836\n",
      "Current training loss is 31.253217697143555\n",
      "Current training loss is 31.24920654296875\n",
      "Current training loss is 31.245206832885742\n",
      "Current training loss is 31.24119758605957\n",
      "Current training loss is 31.237194061279297\n",
      "Current training loss is 31.233192443847656\n",
      "Current training loss is 31.22919273376465\n",
      "Current training loss is 31.22520637512207\n",
      "Current training loss is 31.22121238708496\n",
      "Current training loss is 31.21722412109375\n",
      "Current training loss is 31.21323585510254\n",
      "Current training loss is 31.209247589111328\n",
      "Current training loss is 31.205265045166016\n",
      "Current training loss is 31.201290130615234\n",
      "Current training loss is 31.19731330871582\n",
      "Current training loss is 31.193344116210938\n",
      "Current training loss is 31.189369201660156\n",
      "Current training loss is 31.185409545898438\n",
      "Current training loss is 31.181440353393555\n",
      "Current training loss is 31.177480697631836\n",
      "Current training loss is 31.17351722717285\n",
      "Current training loss is 31.169559478759766\n",
      "Current training loss is 31.165607452392578\n",
      "Current training loss is 31.16165542602539\n",
      "Current training loss is 31.15770149230957\n",
      "Current training loss is 31.153751373291016\n",
      "Current training loss is 31.14981460571289\n",
      "Current training loss is 31.1458740234375\n",
      "Current training loss is 31.14193344116211\n",
      "Current training loss is 31.13799476623535\n",
      "Current training loss is 31.134061813354492\n",
      "Current training loss is 31.1301326751709\n",
      "Current training loss is 31.126203536987305\n",
      "Current training loss is 31.12228012084961\n",
      "Current training loss is 31.118356704711914\n",
      "Current training loss is 31.114437103271484\n",
      "Current training loss is 31.110517501831055\n",
      "Current training loss is 31.106603622436523\n",
      "Current training loss is 31.10269546508789\n",
      "Current training loss is 31.09878921508789\n",
      "Current training loss is 31.094877243041992\n",
      "Current training loss is 31.090970993041992\n",
      "Current training loss is 31.08707046508789\n",
      "Current training loss is 31.083175659179688\n",
      "Current training loss is 31.07927703857422\n",
      "Current training loss is 31.075382232666016\n",
      "Current training loss is 31.071495056152344\n",
      "Current training loss is 31.067604064941406\n",
      "Current training loss is 31.063720703125\n",
      "Current training loss is 31.059837341308594\n",
      "Current training loss is 31.05595588684082\n",
      "Current training loss is 31.052080154418945\n",
      "Current training loss is 31.048208236694336\n",
      "Current training loss is 31.04433822631836\n",
      "Current training loss is 31.040462493896484\n",
      "Current training loss is 31.036598205566406\n",
      "Current training loss is 31.032737731933594\n",
      "Current training loss is 31.028879165649414\n",
      "Current training loss is 31.0250186920166\n",
      "Current training loss is 31.021162033081055\n",
      "Current training loss is 31.017311096191406\n",
      "Current training loss is 31.01346206665039\n",
      "Current training loss is 31.00960922241211\n",
      "Current training loss is 31.005767822265625\n",
      "Current training loss is 31.001928329467773\n",
      "Current training loss is 30.998085021972656\n",
      "Current training loss is 30.99425506591797\n",
      "Current training loss is 30.99041748046875\n",
      "Current training loss is 30.98658561706543\n",
      "Current training loss is 30.982765197753906\n",
      "Current training loss is 30.978933334350586\n",
      "Current training loss is 30.975107192993164\n",
      "Current training loss is 30.97129249572754\n",
      "Current training loss is 30.967470169067383\n",
      "Current training loss is 30.96366310119629\n",
      "Current training loss is 30.959848403930664\n",
      "Current training loss is 30.956037521362305\n",
      "Current training loss is 30.95223045349121\n",
      "Current training loss is 30.948427200317383\n",
      "Current training loss is 30.94462776184082\n",
      "Current training loss is 30.94083023071289\n",
      "Current training loss is 30.93703842163086\n",
      "Current training loss is 30.933246612548828\n",
      "Current training loss is 30.929452896118164\n",
      "Current training loss is 30.925662994384766\n",
      "Current training loss is 30.921878814697266\n",
      "Current training loss is 30.918102264404297\n",
      "Current training loss is 30.91431999206543\n",
      "Current training loss is 30.91054344177246\n",
      "Current training loss is 30.906768798828125\n",
      "Current training loss is 30.90300178527832\n",
      "Current training loss is 30.899232864379883\n",
      "Current training loss is 30.89547348022461\n",
      "Current training loss is 30.891706466674805\n",
      "Current training loss is 30.887948989868164\n",
      "Current training loss is 30.88419532775879\n",
      "Current training loss is 30.880435943603516\n",
      "Current training loss is 30.876686096191406\n",
      "Current training loss is 30.872934341430664\n",
      "Current training loss is 30.86919593811035\n",
      "Current training loss is 30.86545181274414\n",
      "Current training loss is 30.861711502075195\n",
      "Current training loss is 30.857975006103516\n",
      "Current training loss is 30.85424041748047\n",
      "Current training loss is 30.850507736206055\n",
      "Current training loss is 30.84678077697754\n",
      "Current training loss is 30.843046188354492\n",
      "Current training loss is 30.83932876586914\n",
      "Current training loss is 30.835609436035156\n",
      "Current training loss is 30.831892013549805\n",
      "Current training loss is 30.82817840576172\n",
      "Current training loss is 30.824464797973633\n",
      "Current training loss is 30.820751190185547\n",
      "Current training loss is 30.81705093383789\n",
      "Current training loss is 30.813344955444336\n",
      "Current training loss is 30.80964469909668\n",
      "Current training loss is 30.805946350097656\n",
      "Current training loss is 30.8022518157959\n",
      "Current training loss is 30.798559188842773\n",
      "Current training loss is 30.79486656188965\n",
      "Current training loss is 30.791181564331055\n",
      "Current training loss is 30.78749656677246\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current training loss is 30.7838191986084\n",
      "Current training loss is 30.780134201049805\n",
      "Current training loss is 30.776458740234375\n",
      "Current training loss is 30.77278709411621\n",
      "Current training loss is 30.769123077392578\n",
      "Current training loss is 30.765453338623047\n",
      "Current training loss is 30.76178550720215\n",
      "Current training loss is 30.75812339782715\n",
      "Current training loss is 30.754467010498047\n",
      "Current training loss is 30.750810623168945\n",
      "Current training loss is 30.747154235839844\n",
      "Current training loss is 30.74350357055664\n",
      "Current training loss is 30.7398624420166\n",
      "Current training loss is 30.7362117767334\n",
      "Current training loss is 30.732574462890625\n",
      "Current training loss is 30.728933334350586\n",
      "Current training loss is 30.72529411315918\n",
      "Current training loss is 30.721664428710938\n",
      "Current training loss is 30.71803855895996\n",
      "Current training loss is 30.714406967163086\n",
      "Current training loss is 30.710784912109375\n",
      "Current training loss is 30.707162857055664\n",
      "Current training loss is 30.703536987304688\n",
      "Current training loss is 30.69992446899414\n",
      "Current training loss is 30.696308135986328\n",
      "Current training loss is 30.69269371032715\n",
      "Current training loss is 30.689088821411133\n",
      "Current training loss is 30.685487747192383\n",
      "Current training loss is 30.681880950927734\n",
      "Current training loss is 30.678279876708984\n",
      "Current training loss is 30.6746883392334\n",
      "Current training loss is 30.671092987060547\n",
      "Current training loss is 30.667499542236328\n",
      "Current training loss is 30.663915634155273\n",
      "Current training loss is 30.660329818725586\n",
      "Current training loss is 30.656747817993164\n",
      "Current training loss is 30.653167724609375\n",
      "Current training loss is 30.649587631225586\n",
      "Current training loss is 30.64601707458496\n",
      "Current training loss is 30.642446517944336\n",
      "Current training loss is 30.63887596130371\n",
      "Current training loss is 30.635311126708984\n",
      "Current training loss is 30.631750106811523\n",
      "Current training loss is 30.628190994262695\n",
      "Current training loss is 30.624635696411133\n",
      "Current training loss is 30.621078491210938\n",
      "Current training loss is 30.617528915405273\n",
      "Current training loss is 30.613981246948242\n",
      "Current training loss is 30.61043357849121\n",
      "Current training loss is 30.60689353942871\n",
      "Current training loss is 30.603361129760742\n",
      "Current training loss is 30.59981346130371\n",
      "Current training loss is 30.59627914428711\n",
      "Current training loss is 30.59275245666504\n",
      "Current training loss is 30.589223861694336\n",
      "Current training loss is 30.585697174072266\n",
      "Current training loss is 30.582176208496094\n",
      "Current training loss is 30.578657150268555\n",
      "Current training loss is 30.575136184692383\n",
      "Current training loss is 30.57162094116211\n",
      "Current training loss is 30.568117141723633\n",
      "Current training loss is 30.564605712890625\n",
      "Current training loss is 30.561100006103516\n",
      "Current training loss is 30.557594299316406\n",
      "Current training loss is 30.55409812927246\n",
      "Current training loss is 30.550607681274414\n",
      "Current training loss is 30.54711151123047\n",
      "Current training loss is 30.54361915588379\n",
      "Current training loss is 30.54012680053711\n",
      "Current training loss is 30.53664779663086\n",
      "Current training loss is 30.533164978027344\n",
      "Current training loss is 30.52968406677246\n",
      "Current training loss is 30.526206970214844\n",
      "Current training loss is 30.522733688354492\n",
      "Current training loss is 30.519269943237305\n",
      "Current training loss is 30.515798568725586\n",
      "Current training loss is 30.512331008911133\n",
      "Current training loss is 30.508869171142578\n",
      "Current training loss is 30.505409240722656\n",
      "Current training loss is 30.501949310302734\n",
      "Current training loss is 30.498497009277344\n",
      "Current training loss is 30.49505043029785\n",
      "Current training loss is 30.491600036621094\n",
      "Current training loss is 30.488155364990234\n",
      "Current training loss is 30.48471450805664\n",
      "Current training loss is 30.48127555847168\n",
      "Current training loss is 30.47783851623535\n",
      "Current training loss is 30.474403381347656\n",
      "Current training loss is 30.47097396850586\n",
      "Current training loss is 30.467548370361328\n",
      "Current training loss is 30.464120864868164\n",
      "Current training loss is 30.46070671081543\n",
      "Current training loss is 30.4572811126709\n",
      "Current training loss is 30.453868865966797\n",
      "Current training loss is 30.450456619262695\n",
      "Current training loss is 30.447040557861328\n",
      "Current training loss is 30.44363784790039\n",
      "Current training loss is 30.44023323059082\n",
      "Current training loss is 30.43683433532715\n",
      "Current training loss is 30.433433532714844\n",
      "Current training loss is 30.430036544799805\n",
      "Current training loss is 30.426645278930664\n",
      "Current training loss is 30.423254013061523\n",
      "Current training loss is 30.419870376586914\n",
      "Current training loss is 30.416486740112305\n",
      "Current training loss is 30.413105010986328\n",
      "Current training loss is 30.40972137451172\n",
      "Current training loss is 30.406349182128906\n",
      "Current training loss is 30.402976989746094\n",
      "Current training loss is 30.39961051940918\n",
      "Current training loss is 30.396242141723633\n",
      "Current training loss is 30.39287757873535\n",
      "Current training loss is 30.389516830444336\n",
      "Current training loss is 30.386154174804688\n",
      "Current training loss is 30.38280487060547\n",
      "Current training loss is 30.379457473754883\n",
      "Current training loss is 30.376108169555664\n",
      "Current training loss is 30.372760772705078\n",
      "Current training loss is 30.36941909790039\n",
      "Current training loss is 30.36607551574707\n",
      "Current training loss is 30.362735748291016\n",
      "Current training loss is 30.359403610229492\n",
      "Current training loss is 30.35607147216797\n",
      "Current training loss is 30.352739334106445\n",
      "Current training loss is 30.34941864013672\n",
      "Current training loss is 30.346092224121094\n",
      "Current training loss is 30.3427734375\n",
      "Current training loss is 30.33945655822754\n",
      "Current training loss is 30.33614158630371\n",
      "Current training loss is 30.332836151123047\n",
      "Current training loss is 30.32952308654785\n",
      "Current training loss is 30.326217651367188\n",
      "Current training loss is 30.322919845581055\n",
      "Current training loss is 30.31961441040039\n",
      "Current training loss is 30.316314697265625\n",
      "Current training loss is 30.313026428222656\n",
      "Current training loss is 30.309738159179688\n",
      "Current training loss is 30.306447982788086\n",
      "Current training loss is 30.303165435791016\n",
      "Current training loss is 30.299882888793945\n",
      "Current training loss is 30.296606063842773\n",
      "Current training loss is 30.293325424194336\n",
      "Current training loss is 30.29005241394043\n",
      "Current training loss is 30.286781311035156\n",
      "Current training loss is 30.283512115478516\n",
      "Current training loss is 30.280248641967773\n",
      "Current training loss is 30.2769832611084\n",
      "Current training loss is 30.27373504638672\n",
      "Current training loss is 30.270475387573242\n",
      "Current training loss is 30.267229080200195\n",
      "Current training loss is 30.26397132873535\n",
      "Current training loss is 30.260725021362305\n",
      "Current training loss is 30.257484436035156\n",
      "Current training loss is 30.254240036010742\n",
      "Current training loss is 30.251007080078125\n",
      "Current training loss is 30.24776840209961\n",
      "Current training loss is 30.244529724121094\n",
      "Current training loss is 30.241310119628906\n",
      "Current training loss is 30.23807716369629\n",
      "Current training loss is 30.2348575592041\n",
      "Current training loss is 30.231639862060547\n",
      "Current training loss is 30.228416442871094\n",
      "Current training loss is 30.225204467773438\n",
      "Current training loss is 30.221996307373047\n",
      "Current training loss is 30.218788146972656\n",
      "Current training loss is 30.215572357177734\n",
      "Current training loss is 30.21237564086914\n",
      "Current training loss is 30.209178924560547\n",
      "Current training loss is 30.20598602294922\n",
      "Current training loss is 30.202781677246094\n",
      "Current training loss is 30.199594497680664\n",
      "Current training loss is 30.1964054107666\n",
      "Current training loss is 30.19322395324707\n",
      "Current training loss is 30.19003677368164\n",
      "Current training loss is 30.186859130859375\n",
      "Current training loss is 30.183683395385742\n",
      "Current training loss is 30.180511474609375\n",
      "Current training loss is 30.177335739135742\n",
      "Current training loss is 30.17417335510254\n",
      "Current training loss is 30.17100715637207\n",
      "Current training loss is 30.167842864990234\n",
      "Current training loss is 30.164684295654297\n",
      "Current training loss is 30.161529541015625\n",
      "Current training loss is 30.15837860107422\n",
      "Current training loss is 30.15522575378418\n",
      "Current training loss is 30.152076721191406\n",
      "Current training loss is 30.14893913269043\n",
      "Current training loss is 30.145795822143555\n",
      "Current training loss is 30.142656326293945\n",
      "Current training loss is 30.1395206451416\n",
      "Current training loss is 30.136390686035156\n",
      "Current training loss is 30.13326072692871\n",
      "Current training loss is 30.130138397216797\n",
      "Current training loss is 30.127010345458984\n",
      "Current training loss is 30.123891830444336\n",
      "Current training loss is 30.12077522277832\n",
      "Current training loss is 30.117660522460938\n",
      "Current training loss is 30.114545822143555\n",
      "Current training loss is 30.111434936523438\n",
      "Current training loss is 30.108333587646484\n",
      "Current training loss is 30.1052303314209\n",
      "Current training loss is 30.102128982543945\n",
      "Current training loss is 30.09903335571289\n",
      "Current training loss is 30.09593963623047\n",
      "Current training loss is 30.092845916748047\n",
      "Current training loss is 30.08975601196289\n",
      "Current training loss is 30.086673736572266\n",
      "Current training loss is 30.083593368530273\n",
      "Current training loss is 30.080514907836914\n",
      "Current training loss is 30.077436447143555\n",
      "Current training loss is 30.07436180114746\n",
      "Current training loss is 30.071290969848633\n",
      "Current training loss is 30.06822395324707\n",
      "Current training loss is 30.06515884399414\n",
      "Current training loss is 30.06209373474121\n",
      "Current training loss is 30.05904197692871\n",
      "Current training loss is 30.05598258972168\n",
      "Current training loss is 30.05293083190918\n",
      "Current training loss is 30.04987907409668\n",
      "Current training loss is 30.04683494567871\n",
      "Current training loss is 30.043785095214844\n",
      "Current training loss is 30.040748596191406\n",
      "Current training loss is 30.03771209716797\n",
      "Current training loss is 30.0346736907959\n",
      "Current training loss is 30.031646728515625\n",
      "Current training loss is 30.02861785888672\n",
      "Current training loss is 30.02558708190918\n",
      "Current training loss is 30.022567749023438\n",
      "Current training loss is 30.019542694091797\n",
      "Current training loss is 30.016525268554688\n",
      "Current training loss is 30.013511657714844\n",
      "Current training loss is 30.0105037689209\n",
      "Current training loss is 30.00749397277832\n",
      "Current training loss is 30.00448989868164\n",
      "Current training loss is 30.001483917236328\n",
      "Current training loss is 29.998485565185547\n",
      "Current training loss is 29.995485305786133\n",
      "Current training loss is 29.992496490478516\n",
      "Current training loss is 29.989503860473633\n",
      "Current training loss is 29.986513137817383\n",
      "Current training loss is 29.9835262298584\n",
      "Current training loss is 29.980546951293945\n",
      "Current training loss is 29.977567672729492\n",
      "Current training loss is 29.974592208862305\n",
      "Current training loss is 29.971620559692383\n",
      "Current training loss is 29.968647003173828\n",
      "Current training loss is 29.965681076049805\n",
      "Current training loss is 29.962717056274414\n",
      "Current training loss is 29.95975685119629\n",
      "Current training loss is 29.956796646118164\n",
      "Current training loss is 29.953840255737305\n",
      "Current training loss is 29.950885772705078\n",
      "Current training loss is 29.94793701171875\n",
      "Current training loss is 29.94499397277832\n",
      "Current training loss is 29.942045211791992\n",
      "Current training loss is 29.939105987548828\n",
      "Current training loss is 29.93617057800293\n",
      "Current training loss is 29.933237075805664\n",
      "Current training loss is 29.930299758911133\n",
      "Current training loss is 29.9273738861084\n",
      "Current training loss is 29.92445182800293\n",
      "Current training loss is 29.921520233154297\n",
      "Current training loss is 29.91860580444336\n",
      "Current training loss is 29.915685653686523\n",
      "Current training loss is 29.91277503967285\n",
      "Current training loss is 29.90985679626465\n",
      "Current training loss is 29.906951904296875\n",
      "Current training loss is 29.9040470123291\n",
      "Current training loss is 29.901145935058594\n",
      "Current training loss is 29.89824104309082\n",
      "Current training loss is 29.895347595214844\n",
      "Current training loss is 29.892452239990234\n",
      "Current training loss is 29.889562606811523\n",
      "Current training loss is 29.886674880981445\n",
      "Current training loss is 29.883790969848633\n",
      "Current training loss is 29.880905151367188\n",
      "Current training loss is 29.87803077697754\n",
      "Current training loss is 29.87515640258789\n",
      "Current training loss is 29.87228012084961\n",
      "Current training loss is 29.86941146850586\n",
      "Current training loss is 29.866546630859375\n",
      "Current training loss is 29.86368179321289\n",
      "Current training loss is 29.86081886291504\n",
      "Current training loss is 29.857961654663086\n",
      "Current training loss is 29.855106353759766\n",
      "Current training loss is 29.852251052856445\n",
      "Current training loss is 29.849401473999023\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current training loss is 29.8465576171875\n",
      "Current training loss is 29.843708038330078\n",
      "Current training loss is 29.840871810913086\n",
      "Current training loss is 29.838029861450195\n",
      "Current training loss is 29.835195541381836\n",
      "Current training loss is 29.832365036010742\n",
      "Current training loss is 29.82953453063965\n",
      "Current training loss is 29.826705932617188\n",
      "Current training loss is 29.823888778686523\n",
      "Current training loss is 29.821067810058594\n",
      "Current training loss is 29.818246841430664\n",
      "Current training loss is 29.8154354095459\n",
      "Current training loss is 29.812618255615234\n",
      "Current training loss is 29.809814453125\n",
      "Current training loss is 29.807008743286133\n",
      "Current training loss is 29.804210662841797\n",
      "Current training loss is 29.801408767700195\n",
      "Current training loss is 29.798606872558594\n",
      "Current training loss is 29.79581642150879\n",
      "Current training loss is 29.79302406311035\n",
      "Current training loss is 29.790239334106445\n",
      "Current training loss is 29.787452697753906\n",
      "Current training loss is 29.784671783447266\n",
      "Current training loss is 29.781888961791992\n",
      "Current training loss is 29.77911376953125\n",
      "Current training loss is 29.77634048461914\n",
      "Current training loss is 29.7735652923584\n",
      "Current training loss is 29.770803451538086\n",
      "Current training loss is 29.76803970336914\n",
      "Current training loss is 29.765277862548828\n",
      "Current training loss is 29.762516021728516\n",
      "Current training loss is 29.7597599029541\n",
      "Current training loss is 29.75701141357422\n",
      "Current training loss is 29.754255294799805\n",
      "Current training loss is 29.751508712768555\n",
      "Current training loss is 29.74877166748047\n",
      "Current training loss is 29.746028900146484\n",
      "Current training loss is 29.743288040161133\n",
      "Current training loss is 29.740556716918945\n",
      "Current training loss is 29.737823486328125\n",
      "Current training loss is 29.735092163085938\n",
      "Current training loss is 29.732364654541016\n",
      "Current training loss is 29.729642868041992\n",
      "Current training loss is 29.72692108154297\n",
      "Current training loss is 29.724201202392578\n",
      "Current training loss is 29.72149085998535\n",
      "Current training loss is 29.718780517578125\n",
      "Current training loss is 29.716066360473633\n",
      "Current training loss is 29.713369369506836\n",
      "Current training loss is 29.710660934448242\n",
      "Current training loss is 29.70796012878418\n",
      "Current training loss is 29.705263137817383\n",
      "Current training loss is 29.70256805419922\n",
      "Current training loss is 29.699874877929688\n",
      "Current training loss is 29.697187423706055\n",
      "Current training loss is 29.694501876831055\n",
      "Current training loss is 29.691818237304688\n",
      "Current training loss is 29.68914031982422\n",
      "Current training loss is 29.68646240234375\n",
      "Current training loss is 29.683788299560547\n",
      "Current training loss is 29.68111801147461\n",
      "Current training loss is 29.678449630737305\n",
      "Current training loss is 29.6757869720459\n",
      "Current training loss is 29.673124313354492\n",
      "Current training loss is 29.670461654663086\n",
      "Current training loss is 29.66781234741211\n",
      "Current training loss is 29.6651554107666\n",
      "Current training loss is 29.662498474121094\n",
      "Current training loss is 29.65985679626465\n",
      "Current training loss is 29.657211303710938\n",
      "Current training loss is 29.65456771850586\n",
      "Current training loss is 29.651927947998047\n",
      "Current training loss is 29.649295806884766\n",
      "Current training loss is 29.646657943725586\n",
      "Current training loss is 29.64402961730957\n",
      "Current training loss is 29.641403198242188\n",
      "Current training loss is 29.638778686523438\n",
      "Current training loss is 29.636159896850586\n",
      "Current training loss is 29.63353729248047\n",
      "Current training loss is 29.63092041015625\n",
      "Current training loss is 29.628307342529297\n",
      "Current training loss is 29.62569808959961\n",
      "Current training loss is 29.623098373413086\n",
      "Current training loss is 29.6204891204834\n",
      "Current training loss is 29.617887496948242\n",
      "Current training loss is 29.615285873413086\n",
      "Current training loss is 29.61269760131836\n",
      "Current training loss is 29.610107421875\n",
      "Current training loss is 29.607513427734375\n",
      "Current training loss is 29.604928970336914\n",
      "Current training loss is 29.60234260559082\n",
      "Current training loss is 29.599761962890625\n",
      "Current training loss is 29.597187042236328\n",
      "Current training loss is 29.594608306884766\n",
      "Current training loss is 29.592041015625\n",
      "Current training loss is 29.58946990966797\n",
      "Current training loss is 29.58690071105957\n",
      "Current training loss is 29.584335327148438\n",
      "Current training loss is 29.581771850585938\n",
      "Current training loss is 29.579219818115234\n",
      "Current training loss is 29.576663970947266\n",
      "Current training loss is 29.57411003112793\n",
      "Current training loss is 29.571561813354492\n",
      "Current training loss is 29.56901741027832\n",
      "Current training loss is 29.56647300720215\n",
      "Current training loss is 29.563934326171875\n",
      "Current training loss is 29.561391830444336\n",
      "Current training loss is 29.558860778808594\n",
      "Current training loss is 29.55632781982422\n",
      "Current training loss is 29.553800582885742\n",
      "Current training loss is 29.551271438598633\n",
      "Current training loss is 29.54874610900879\n",
      "Current training loss is 29.546232223510742\n",
      "Current training loss is 29.543710708618164\n",
      "Current training loss is 29.541194915771484\n",
      "Current training loss is 29.538686752319336\n",
      "Current training loss is 29.536178588867188\n",
      "Current training loss is 29.533674240112305\n",
      "Current training loss is 29.531171798706055\n",
      "Current training loss is 29.52867317199707\n",
      "Current training loss is 29.52617073059082\n",
      "Current training loss is 29.5236759185791\n",
      "Current training loss is 29.52118492126465\n",
      "Current training loss is 29.518699645996094\n",
      "Current training loss is 29.516212463378906\n",
      "Current training loss is 29.51373291015625\n",
      "Current training loss is 29.51124382019043\n",
      "Current training loss is 29.508771896362305\n",
      "Current training loss is 29.506298065185547\n",
      "Current training loss is 29.503828048706055\n",
      "Current training loss is 29.501361846923828\n",
      "Current training loss is 29.49889373779297\n",
      "Current training loss is 29.496429443359375\n",
      "Current training loss is 29.493968963623047\n",
      "Current training loss is 29.49151611328125\n",
      "Current training loss is 29.489059448242188\n",
      "Current training loss is 29.486608505249023\n",
      "Current training loss is 29.484161376953125\n",
      "Current training loss is 29.48171615600586\n",
      "Current training loss is 29.479270935058594\n",
      "Current training loss is 29.47683334350586\n",
      "Current training loss is 29.474393844604492\n",
      "Current training loss is 29.471960067749023\n",
      "Current training loss is 29.469533920288086\n",
      "Current training loss is 29.46710205078125\n",
      "Current training loss is 29.464672088623047\n",
      "Current training loss is 29.46225357055664\n",
      "Current training loss is 29.45983123779297\n",
      "Current training loss is 29.457416534423828\n",
      "Current training loss is 29.45500373840332\n",
      "Current training loss is 29.452587127685547\n",
      "Current training loss is 29.45018196105957\n",
      "Current training loss is 29.44777488708496\n",
      "Current training loss is 29.445375442504883\n",
      "Current training loss is 29.44297218322754\n",
      "Current training loss is 29.440574645996094\n",
      "Current training loss is 29.438180923461914\n",
      "Current training loss is 29.435792922973633\n",
      "Current training loss is 29.43340492248535\n",
      "Current training loss is 29.431015014648438\n",
      "Current training loss is 29.428634643554688\n",
      "Current training loss is 29.42625617980957\n",
      "Current training loss is 29.42388153076172\n",
      "Current training loss is 29.4215030670166\n",
      "Current training loss is 29.419130325317383\n",
      "Current training loss is 29.41676139831543\n",
      "Current training loss is 29.414396286010742\n",
      "Current training loss is 29.41203498840332\n",
      "Current training loss is 29.409669876098633\n",
      "Current training loss is 29.407316207885742\n",
      "Current training loss is 29.40496063232422\n",
      "Current training loss is 29.402616500854492\n",
      "Current training loss is 29.4002628326416\n",
      "Current training loss is 29.397916793823242\n",
      "Current training loss is 29.39556884765625\n",
      "Current training loss is 29.393234252929688\n",
      "Current training loss is 29.390899658203125\n",
      "Current training loss is 29.388561248779297\n",
      "Current training loss is 29.38623046875\n",
      "Current training loss is 29.383901596069336\n",
      "Current training loss is 29.381568908691406\n",
      "Current training loss is 29.379249572753906\n",
      "Current training loss is 29.376928329467773\n",
      "Current training loss is 29.374610900878906\n",
      "Current training loss is 29.372297286987305\n",
      "Current training loss is 29.36998748779297\n",
      "Current training loss is 29.36767578125\n",
      "Current training loss is 29.365367889404297\n",
      "Current training loss is 29.363067626953125\n",
      "Current training loss is 29.36076545715332\n",
      "Current training loss is 29.35846519470215\n",
      "Current training loss is 29.356170654296875\n",
      "Current training loss is 29.353878021240234\n",
      "Current training loss is 29.351585388183594\n",
      "Current training loss is 29.349306106567383\n",
      "Current training loss is 29.34701919555664\n",
      "Current training loss is 29.34473991394043\n",
      "Current training loss is 29.342458724975586\n",
      "Current training loss is 29.340179443359375\n",
      "Current training loss is 29.337907791137695\n",
      "Current training loss is 29.33563804626465\n",
      "Current training loss is 29.3333740234375\n",
      "Current training loss is 29.33111000061035\n",
      "Current training loss is 29.328847885131836\n",
      "Current training loss is 29.326583862304688\n",
      "Current training loss is 29.3243350982666\n",
      "Current training loss is 29.322084426879883\n",
      "Current training loss is 29.3198299407959\n",
      "Current training loss is 29.317584991455078\n",
      "Current training loss is 29.315336227416992\n",
      "Current training loss is 29.313093185424805\n",
      "Current training loss is 29.31085777282715\n",
      "Current training loss is 29.30861473083496\n",
      "Current training loss is 29.3063907623291\n",
      "Current training loss is 29.30415153503418\n",
      "Current training loss is 29.30192756652832\n",
      "Current training loss is 29.299701690673828\n",
      "Current training loss is 29.2974796295166\n",
      "Current training loss is 29.295263290405273\n",
      "Current training loss is 29.293039321899414\n",
      "Current training loss is 29.29082679748535\n",
      "Current training loss is 29.288618087768555\n",
      "Current training loss is 29.286405563354492\n",
      "Current training loss is 29.28420639038086\n",
      "Current training loss is 29.28199577331543\n",
      "Current training loss is 29.279794692993164\n",
      "Current training loss is 29.277599334716797\n",
      "Current training loss is 29.27540397644043\n",
      "Current training loss is 29.27322006225586\n",
      "Current training loss is 29.271024703979492\n",
      "Current training loss is 29.268842697143555\n",
      "Current training loss is 29.26664924621582\n",
      "Current training loss is 29.264469146728516\n",
      "Current training loss is 29.26229476928711\n",
      "Current training loss is 29.26012420654297\n",
      "Current training loss is 29.257944107055664\n",
      "Current training loss is 29.255775451660156\n",
      "Current training loss is 29.25360679626465\n",
      "Current training loss is 29.25144386291504\n",
      "Current training loss is 29.249286651611328\n",
      "Current training loss is 29.24712371826172\n",
      "Current training loss is 29.244970321655273\n",
      "Current training loss is 29.242816925048828\n",
      "Current training loss is 29.240663528442383\n",
      "Current training loss is 29.238515853881836\n",
      "Current training loss is 29.236371994018555\n",
      "Current training loss is 29.23423194885254\n",
      "Current training loss is 29.232091903686523\n",
      "Current training loss is 29.229949951171875\n",
      "Current training loss is 29.227811813354492\n",
      "Current training loss is 29.22568702697754\n",
      "Current training loss is 29.223556518554688\n",
      "Current training loss is 29.221435546875\n",
      "Current training loss is 29.21931266784668\n",
      "Current training loss is 29.217191696166992\n",
      "Current training loss is 29.21507453918457\n",
      "Current training loss is 29.21295738220215\n",
      "Current training loss is 29.210844039916992\n",
      "Current training loss is 29.208740234375\n",
      "Current training loss is 29.20663070678711\n",
      "Current training loss is 29.20452308654785\n",
      "Current training loss is 29.202421188354492\n",
      "Current training loss is 29.2003231048584\n",
      "Current training loss is 29.19822883605957\n",
      "Current training loss is 29.196142196655273\n",
      "Current training loss is 29.194042205810547\n",
      "Current training loss is 29.191957473754883\n",
      "Current training loss is 29.18987274169922\n",
      "Current training loss is 29.187782287597656\n",
      "Current training loss is 29.18570899963379\n",
      "Current training loss is 29.183637619018555\n",
      "Current training loss is 29.18155860900879\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current training loss is 29.179487228393555\n",
      "Current training loss is 29.17741584777832\n",
      "Current training loss is 29.175350189208984\n",
      "Current training loss is 29.173290252685547\n",
      "Current training loss is 29.171226501464844\n",
      "Current training loss is 29.16917610168457\n",
      "Current training loss is 29.1671199798584\n",
      "Current training loss is 29.16506004333496\n",
      "Current training loss is 29.163015365600586\n",
      "Current training loss is 29.16096305847168\n",
      "Current training loss is 29.158926010131836\n",
      "Current training loss is 29.156879425048828\n",
      "Current training loss is 29.154842376708984\n",
      "Current training loss is 29.15280532836914\n",
      "Current training loss is 29.150775909423828\n",
      "Current training loss is 29.148738861083984\n",
      "Current training loss is 29.146711349487305\n",
      "Current training loss is 29.144689559936523\n",
      "Current training loss is 29.14266586303711\n",
      "Current training loss is 29.140644073486328\n",
      "Current training loss is 29.13862419128418\n",
      "Current training loss is 29.13661003112793\n",
      "Current training loss is 29.134599685668945\n",
      "Current training loss is 29.13258934020996\n",
      "Current training loss is 29.130582809448242\n",
      "Current training loss is 29.128576278686523\n",
      "Current training loss is 29.12657928466797\n",
      "Current training loss is 29.12457847595215\n",
      "Current training loss is 29.122581481933594\n",
      "Current training loss is 29.120588302612305\n",
      "Current training loss is 29.118595123291016\n",
      "Current training loss is 29.116615295410156\n",
      "Current training loss is 29.1146240234375\n",
      "Current training loss is 29.112646102905273\n",
      "Current training loss is 29.110658645629883\n",
      "Current training loss is 29.108680725097656\n",
      "Current training loss is 29.10671043395996\n",
      "Current training loss is 29.104738235473633\n",
      "Current training loss is 29.102767944335938\n",
      "Current training loss is 29.10080337524414\n",
      "Current training loss is 29.098834991455078\n",
      "Current training loss is 29.09687614440918\n",
      "Current training loss is 29.09491539001465\n",
      "Current training loss is 29.09296226501465\n",
      "Current training loss is 29.091005325317383\n",
      "Current training loss is 29.089054107666016\n",
      "Current training loss is 29.087106704711914\n",
      "Current training loss is 29.085163116455078\n",
      "Current training loss is 29.083221435546875\n",
      "Current training loss is 29.08127784729004\n",
      "Current training loss is 29.0793399810791\n",
      "Current training loss is 29.07740592956543\n",
      "Current training loss is 29.07547378540039\n",
      "Current training loss is 29.073549270629883\n",
      "Current training loss is 29.07162094116211\n",
      "Current training loss is 29.0696964263916\n",
      "Current training loss is 29.06777000427246\n",
      "Current training loss is 29.06585121154785\n",
      "Current training loss is 29.063940048217773\n",
      "Current training loss is 29.06202507019043\n",
      "Current training loss is 29.060110092163086\n",
      "Current training loss is 29.05820083618164\n",
      "Current training loss is 29.056297302246094\n",
      "Current training loss is 29.05439567565918\n",
      "Current training loss is 29.052490234375\n",
      "Current training loss is 29.050594329833984\n",
      "Current training loss is 29.0487003326416\n",
      "Current training loss is 29.04680824279785\n",
      "Current training loss is 29.044918060302734\n",
      "Current training loss is 29.04302978515625\n",
      "Current training loss is 29.041147232055664\n",
      "Current training loss is 29.03925895690918\n",
      "Current training loss is 29.037376403808594\n",
      "Current training loss is 29.035507202148438\n",
      "Current training loss is 29.033626556396484\n",
      "Current training loss is 29.031761169433594\n",
      "Current training loss is 29.02988624572754\n",
      "Current training loss is 29.02802085876465\n",
      "Current training loss is 29.026159286499023\n",
      "Current training loss is 29.024295806884766\n",
      "Current training loss is 29.022430419921875\n",
      "Current training loss is 29.020578384399414\n",
      "Current training loss is 29.01872444152832\n",
      "Current training loss is 29.016874313354492\n",
      "Current training loss is 29.015026092529297\n",
      "Current training loss is 29.013185501098633\n",
      "Current training loss is 29.011329650878906\n",
      "Current training loss is 29.009490966796875\n",
      "Current training loss is 29.00765609741211\n",
      "Current training loss is 29.005821228027344\n",
      "Current training loss is 29.00398826599121\n",
      "Current training loss is 29.002155303955078\n",
      "Current training loss is 29.000333786010742\n",
      "Current training loss is 28.998504638671875\n",
      "Current training loss is 28.996679306030273\n",
      "Current training loss is 28.99485969543457\n",
      "Current training loss is 28.9930419921875\n",
      "Current training loss is 28.99123191833496\n",
      "Current training loss is 28.989418029785156\n",
      "Current training loss is 28.987606048583984\n",
      "Current training loss is 28.985801696777344\n",
      "Current training loss is 28.983993530273438\n",
      "Current training loss is 28.98219108581543\n",
      "Current training loss is 28.98039436340332\n",
      "Current training loss is 28.97859764099121\n",
      "Current training loss is 28.97679901123047\n",
      "Current training loss is 28.975006103515625\n",
      "Current training loss is 28.973217010498047\n",
      "Current training loss is 28.971435546875\n",
      "Current training loss is 28.96964454650879\n",
      "Current training loss is 28.967863082885742\n",
      "Current training loss is 28.966081619262695\n",
      "Current training loss is 28.964305877685547\n",
      "Current training loss is 28.962533950805664\n",
      "Current training loss is 28.960763931274414\n",
      "Current training loss is 28.958993911743164\n",
      "Current training loss is 28.957225799560547\n",
      "Current training loss is 28.955461502075195\n",
      "Current training loss is 28.953697204589844\n",
      "Current training loss is 28.951940536499023\n",
      "Current training loss is 28.95018768310547\n",
      "Current training loss is 28.948429107666016\n",
      "Current training loss is 28.94668197631836\n",
      "Current training loss is 28.944929122924805\n",
      "Current training loss is 28.943185806274414\n",
      "Current training loss is 28.941444396972656\n",
      "Current training loss is 28.939693450927734\n",
      "Current training loss is 28.937957763671875\n",
      "Current training loss is 28.93622589111328\n",
      "Current training loss is 28.934492111206055\n",
      "Current training loss is 28.932758331298828\n",
      "Current training loss is 28.931026458740234\n",
      "Current training loss is 28.92930793762207\n",
      "Current training loss is 28.927579879760742\n",
      "Current training loss is 28.925859451293945\n",
      "Current training loss is 28.924144744873047\n",
      "Current training loss is 28.92241668701172\n",
      "Current training loss is 28.92070960998535\n",
      "Current training loss is 28.91899871826172\n",
      "Current training loss is 28.91728973388672\n",
      "Current training loss is 28.91558265686035\n",
      "Current training loss is 28.91387939453125\n",
      "Current training loss is 28.912179946899414\n",
      "Current training loss is 28.910478591918945\n",
      "Current training loss is 28.908781051635742\n",
      "Current training loss is 28.90709114074707\n",
      "Current training loss is 28.9053955078125\n",
      "Current training loss is 28.90370750427246\n",
      "Current training loss is 28.902023315429688\n",
      "Current training loss is 28.90033721923828\n",
      "Current training loss is 28.898656845092773\n",
      "Current training loss is 28.896984100341797\n",
      "Current training loss is 28.89530372619629\n",
      "Current training loss is 28.89362907409668\n",
      "Current training loss is 28.89195442199707\n",
      "Current training loss is 28.89028549194336\n",
      "Current training loss is 28.888622283935547\n",
      "Current training loss is 28.8869571685791\n",
      "Current training loss is 28.885297775268555\n",
      "Current training loss is 28.883634567260742\n",
      "Current training loss is 28.88197898864746\n",
      "Current training loss is 28.880329132080078\n",
      "Current training loss is 28.878673553466797\n",
      "Current training loss is 28.877025604248047\n",
      "Current training loss is 28.87537384033203\n",
      "Current training loss is 28.87373161315918\n",
      "Current training loss is 28.872089385986328\n",
      "Current training loss is 28.870452880859375\n",
      "Current training loss is 28.868810653686523\n",
      "Current training loss is 28.867177963256836\n",
      "Current training loss is 28.86554718017578\n",
      "Current training loss is 28.86391830444336\n",
      "Current training loss is 28.862289428710938\n",
      "Current training loss is 28.860666275024414\n",
      "Current training loss is 28.85904312133789\n",
      "Current training loss is 28.857418060302734\n",
      "Current training loss is 28.855802536010742\n",
      "Current training loss is 28.854183197021484\n",
      "Current training loss is 28.85257339477539\n",
      "Current training loss is 28.85096549987793\n",
      "Current training loss is 28.8493595123291\n",
      "Current training loss is 28.84775733947754\n",
      "Current training loss is 28.84615135192871\n",
      "Current training loss is 28.84455108642578\n",
      "Current training loss is 28.842947006225586\n",
      "Current training loss is 28.841352462768555\n",
      "Current training loss is 28.83976173400879\n",
      "Current training loss is 28.838171005249023\n",
      "Current training loss is 28.836578369140625\n",
      "Current training loss is 28.83499526977539\n",
      "Current training loss is 28.833410263061523\n",
      "Current training loss is 28.83182716369629\n",
      "Current training loss is 28.830251693725586\n",
      "Current training loss is 28.82867431640625\n",
      "Current training loss is 28.827098846435547\n",
      "Current training loss is 28.82552719116211\n",
      "Current training loss is 28.82395362854004\n",
      "Current training loss is 28.822391510009766\n",
      "Current training loss is 28.820829391479492\n",
      "Current training loss is 28.81926155090332\n",
      "Current training loss is 28.817705154418945\n",
      "Current training loss is 28.816146850585938\n",
      "Current training loss is 28.81459617614746\n",
      "Current training loss is 28.81304168701172\n",
      "Current training loss is 28.811485290527344\n",
      "Current training loss is 28.809946060180664\n",
      "Current training loss is 28.80838966369629\n",
      "Current training loss is 28.806859970092773\n",
      "Current training loss is 28.805316925048828\n",
      "Current training loss is 28.803773880004883\n",
      "Current training loss is 28.80223846435547\n",
      "Current training loss is 28.80070686340332\n",
      "Current training loss is 28.799179077148438\n",
      "Current training loss is 28.797643661499023\n",
      "Current training loss is 28.796117782592773\n",
      "Current training loss is 28.794599533081055\n",
      "Current training loss is 28.79307746887207\n",
      "Current training loss is 28.791555404663086\n",
      "Current training loss is 28.790035247802734\n",
      "Current training loss is 28.788522720336914\n",
      "Current training loss is 28.787010192871094\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current training loss is 28.785499572753906\n",
      "Current training loss is 28.78399085998535\n",
      "Current training loss is 28.782487869262695\n",
      "Current training loss is 28.780981063842773\n",
      "Current training loss is 28.779481887817383\n",
      "Current training loss is 28.77798843383789\n",
      "Current training loss is 28.7764892578125\n",
      "Current training loss is 28.774991989135742\n",
      "Current training loss is 28.773500442504883\n",
      "Current training loss is 28.77201271057129\n",
      "Current training loss is 28.77052879333496\n",
      "Current training loss is 28.769046783447266\n",
      "Current training loss is 28.767562866210938\n",
      "Current training loss is 28.766082763671875\n",
      "Current training loss is 28.764606475830078\n",
      "Current training loss is 28.76313018798828\n",
      "Current training loss is 28.76165771484375\n",
      "Current training loss is 28.76018714904785\n",
      "Current training loss is 28.758718490600586\n",
      "Current training loss is 28.75724983215332\n",
      "Current training loss is 28.75579071044922\n",
      "Current training loss is 28.754329681396484\n",
      "Current training loss is 28.752870559692383\n",
      "Current training loss is 28.75141143798828\n",
      "Current training loss is 28.74995994567871\n",
      "Current training loss is 28.74850845336914\n",
      "Current training loss is 28.74705696105957\n",
      "Current training loss is 28.74560546875\n",
      "Current training loss is 28.744171142578125\n",
      "Current training loss is 28.74272346496582\n",
      "Current training loss is 28.741283416748047\n",
      "Current training loss is 28.739843368530273\n",
      "Current training loss is 28.738405227661133\n",
      "Current training loss is 28.736974716186523\n",
      "Current training loss is 28.73554229736328\n",
      "Current training loss is 28.734113693237305\n",
      "Current training loss is 28.732685089111328\n",
      "Current training loss is 28.731264114379883\n",
      "Current training loss is 28.729843139648438\n",
      "Current training loss is 28.728422164916992\n",
      "Current training loss is 28.727006912231445\n",
      "Current training loss is 28.725587844848633\n",
      "Current training loss is 28.72417449951172\n",
      "Current training loss is 28.722768783569336\n",
      "Current training loss is 28.721351623535156\n",
      "Current training loss is 28.719947814941406\n",
      "Current training loss is 28.718544006347656\n",
      "Current training loss is 28.71714210510254\n",
      "Current training loss is 28.71574592590332\n",
      "Current training loss is 28.714345932006836\n",
      "Current training loss is 28.712953567504883\n",
      "Current training loss is 28.71155548095703\n",
      "Current training loss is 28.710166931152344\n",
      "Current training loss is 28.708784103393555\n",
      "Current training loss is 28.7073917388916\n",
      "Current training loss is 28.706010818481445\n",
      "Current training loss is 28.70462989807129\n",
      "Current training loss is 28.7032470703125\n",
      "Current training loss is 28.701871871948242\n",
      "Current training loss is 28.70049476623535\n",
      "Current training loss is 28.69912338256836\n",
      "Current training loss is 28.697750091552734\n",
      "Current training loss is 28.696386337280273\n",
      "Current training loss is 28.695016860961914\n",
      "Current training loss is 28.693655014038086\n",
      "Current training loss is 28.692296981811523\n",
      "Current training loss is 28.690935134887695\n",
      "Current training loss is 28.6895751953125\n",
      "Current training loss is 28.68821907043457\n",
      "Current training loss is 28.686874389648438\n",
      "Current training loss is 28.685516357421875\n",
      "Current training loss is 28.684171676635742\n",
      "Current training loss is 28.68282699584961\n",
      "Current training loss is 28.68148422241211\n",
      "Current training loss is 28.68014144897461\n",
      "Current training loss is 28.67880630493164\n",
      "Current training loss is 28.677465438842773\n",
      "Current training loss is 28.676130294799805\n",
      "Current training loss is 28.674795150756836\n",
      "Current training loss is 28.673471450805664\n",
      "Current training loss is 28.672138214111328\n",
      "Current training loss is 28.670812606811523\n",
      "Current training loss is 28.66948890686035\n",
      "Current training loss is 28.668163299560547\n",
      "Current training loss is 28.666839599609375\n",
      "Current training loss is 28.665529251098633\n",
      "Current training loss is 28.664213180541992\n",
      "Current training loss is 28.66289520263672\n",
      "Current training loss is 28.661588668823242\n",
      "Current training loss is 28.6602783203125\n",
      "Current training loss is 28.658971786499023\n",
      "Current training loss is 28.657672882080078\n",
      "Current training loss is 28.65636444091797\n",
      "Current training loss is 28.655067443847656\n",
      "Current training loss is 28.653770446777344\n",
      "Current training loss is 28.65247344970703\n",
      "Current training loss is 28.65117645263672\n",
      "Current training loss is 28.649887084960938\n",
      "Current training loss is 28.648597717285156\n",
      "Current training loss is 28.647308349609375\n",
      "Current training loss is 28.64603042602539\n",
      "Current training loss is 28.644742965698242\n",
      "Current training loss is 28.643461227416992\n",
      "Current training loss is 28.64218521118164\n",
      "Current training loss is 28.64090919494629\n",
      "Current training loss is 28.639631271362305\n",
      "Current training loss is 28.63836097717285\n",
      "Current training loss is 28.637094497680664\n",
      "Current training loss is 28.63582420349121\n",
      "Current training loss is 28.634559631347656\n",
      "Current training loss is 28.6332950592041\n",
      "Current training loss is 28.632036209106445\n",
      "Current training loss is 28.63077163696289\n",
      "Current training loss is 28.629518508911133\n",
      "Current training loss is 28.62826156616211\n",
      "Current training loss is 28.62700843811035\n",
      "Current training loss is 28.625755310058594\n",
      "Current training loss is 28.624507904052734\n",
      "Current training loss is 28.62326431274414\n",
      "Current training loss is 28.62201690673828\n",
      "Current training loss is 28.62077522277832\n",
      "Current training loss is 28.619537353515625\n",
      "Current training loss is 28.61829948425293\n",
      "Current training loss is 28.6170597076416\n",
      "Current training loss is 28.61583137512207\n",
      "Current training loss is 28.614591598510742\n",
      "Current training loss is 28.613361358642578\n",
      "Current training loss is 28.61213493347168\n",
      "Current training loss is 28.61090850830078\n",
      "Current training loss is 28.609689712524414\n",
      "Current training loss is 28.608470916748047\n",
      "Current training loss is 28.60724639892578\n",
      "Current training loss is 28.606029510498047\n",
      "Current training loss is 28.60481834411621\n",
      "Current training loss is 28.60360336303711\n",
      "Current training loss is 28.60239028930664\n",
      "Current training loss is 28.601179122924805\n",
      "Current training loss is 28.599977493286133\n",
      "Current training loss is 28.598773956298828\n",
      "Current training loss is 28.59756851196289\n",
      "Current training loss is 28.59636688232422\n",
      "Current training loss is 28.595172882080078\n",
      "Current training loss is 28.593971252441406\n",
      "Current training loss is 28.59278106689453\n",
      "Current training loss is 28.591581344604492\n",
      "Current training loss is 28.590394973754883\n",
      "Current training loss is 28.589202880859375\n",
      "Current training loss is 28.58802032470703\n",
      "Current training loss is 28.586835861206055\n",
      "Current training loss is 28.585655212402344\n",
      "Current training loss is 28.584474563598633\n",
      "Current training loss is 28.583297729492188\n",
      "Current training loss is 28.582120895385742\n",
      "Current training loss is 28.580949783325195\n",
      "Current training loss is 28.579776763916016\n",
      "Current training loss is 28.5786075592041\n",
      "Current training loss is 28.577436447143555\n",
      "Current training loss is 28.57627296447754\n",
      "Current training loss is 28.575109481811523\n",
      "Current training loss is 28.57394790649414\n",
      "Current training loss is 28.572784423828125\n",
      "Current training loss is 28.571630477905273\n",
      "Current training loss is 28.570472717285156\n",
      "Current training loss is 28.569320678710938\n",
      "Current training loss is 28.568166732788086\n",
      "Current training loss is 28.5670166015625\n",
      "Current training loss is 28.56587028503418\n",
      "Current training loss is 28.564725875854492\n",
      "Current training loss is 28.563583374023438\n",
      "Current training loss is 28.56243896484375\n",
      "Current training loss is 28.561302185058594\n",
      "Current training loss is 28.560165405273438\n",
      "Current training loss is 28.559030532836914\n",
      "Current training loss is 28.55789566040039\n",
      "Current training loss is 28.556764602661133\n",
      "Current training loss is 28.55563735961914\n",
      "Current training loss is 28.554508209228516\n",
      "Current training loss is 28.553380966186523\n",
      "Current training loss is 28.55225944519043\n",
      "Current training loss is 28.551132202148438\n",
      "Current training loss is 28.55001449584961\n",
      "Current training loss is 28.548900604248047\n",
      "Current training loss is 28.54778480529785\n",
      "Current training loss is 28.546669006347656\n",
      "Current training loss is 28.54555892944336\n",
      "Current training loss is 28.544445037841797\n",
      "Current training loss is 28.543338775634766\n",
      "Current training loss is 28.5422306060791\n",
      "Current training loss is 28.5411319732666\n",
      "Current training loss is 28.54003143310547\n",
      "Current training loss is 28.53892707824707\n",
      "Current training loss is 28.53782844543457\n",
      "Current training loss is 28.5367374420166\n",
      "Current training loss is 28.5356388092041\n",
      "Current training loss is 28.534549713134766\n",
      "Current training loss is 28.53346061706543\n",
      "Current training loss is 28.53237533569336\n",
      "Current training loss is 28.531286239624023\n",
      "Current training loss is 28.53019905090332\n",
      "Current training loss is 28.52912139892578\n",
      "Current training loss is 28.52804183959961\n",
      "Current training loss is 28.52696418762207\n",
      "Current training loss is 28.525882720947266\n",
      "Current training loss is 28.524808883666992\n",
      "Current training loss is 28.52373695373535\n",
      "Current training loss is 28.52267074584961\n",
      "Current training loss is 28.52159881591797\n",
      "Current training loss is 28.520530700683594\n",
      "Current training loss is 28.519466400146484\n",
      "Current training loss is 28.51840591430664\n",
      "Current training loss is 28.517345428466797\n",
      "Current training loss is 28.516286849975586\n",
      "Current training loss is 28.515228271484375\n",
      "Current training loss is 28.51417350769043\n",
      "Current training loss is 28.513118743896484\n",
      "Current training loss is 28.51206398010254\n",
      "Current training loss is 28.51101303100586\n",
      "Current training loss is 28.509965896606445\n",
      "Current training loss is 28.508922576904297\n",
      "Current training loss is 28.50788116455078\n",
      "Current training loss is 28.506839752197266\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current training loss is 28.505800247192383\n",
      "Current training loss is 28.504756927490234\n",
      "Current training loss is 28.50372314453125\n",
      "Current training loss is 28.502689361572266\n",
      "Current training loss is 28.50165557861328\n",
      "Current training loss is 28.50062370300293\n",
      "Current training loss is 28.499595642089844\n",
      "Current training loss is 28.498571395874023\n",
      "Current training loss is 28.49755096435547\n",
      "Current training loss is 28.496524810791016\n",
      "Current training loss is 28.49550437927246\n",
      "Current training loss is 28.494483947753906\n",
      "Current training loss is 28.493465423583984\n",
      "Current training loss is 28.49245262145996\n",
      "Current training loss is 28.49144172668457\n",
      "Current training loss is 28.49042510986328\n",
      "Current training loss is 28.489416122436523\n",
      "Current training loss is 28.488407135009766\n",
      "Current training loss is 28.487401962280273\n",
      "Current training loss is 28.486392974853516\n",
      "Current training loss is 28.485395431518555\n",
      "Current training loss is 28.484392166137695\n",
      "Current training loss is 28.483394622802734\n",
      "Current training loss is 28.482398986816406\n",
      "Current training loss is 28.48140525817871\n",
      "Current training loss is 28.480409622192383\n",
      "Current training loss is 28.479421615600586\n",
      "Current training loss is 28.47842788696289\n",
      "Current training loss is 28.477445602416992\n",
      "Current training loss is 28.476457595825195\n",
      "Current training loss is 28.475467681884766\n",
      "Current training loss is 28.47449493408203\n",
      "Current training loss is 28.4735107421875\n",
      "Current training loss is 28.4725284576416\n",
      "Current training loss is 28.471553802490234\n",
      "Current training loss is 28.470577239990234\n",
      "Current training loss is 28.4696044921875\n",
      "Current training loss is 28.468639373779297\n",
      "Current training loss is 28.46766471862793\n",
      "Current training loss is 28.46669578552246\n",
      "Current training loss is 28.465736389160156\n",
      "Current training loss is 28.464767456054688\n",
      "Current training loss is 28.463802337646484\n",
      "Current training loss is 28.462848663330078\n",
      "Current training loss is 28.461889266967773\n",
      "Current training loss is 28.4609317779541\n",
      "Current training loss is 28.459978103637695\n",
      "Current training loss is 28.459022521972656\n",
      "Current training loss is 28.458070755004883\n",
      "Current training loss is 28.45711898803711\n",
      "Current training loss is 28.4561767578125\n",
      "Current training loss is 28.455228805541992\n",
      "Current training loss is 28.45428466796875\n",
      "Current training loss is 28.453338623046875\n",
      "Current training loss is 28.452396392822266\n",
      "Current training loss is 28.45145606994629\n",
      "Current training loss is 28.45052719116211\n",
      "Current training loss is 28.449588775634766\n",
      "Current training loss is 28.44865608215332\n",
      "Current training loss is 28.447723388671875\n",
      "Current training loss is 28.44679069519043\n",
      "Current training loss is 28.44586944580078\n",
      "Current training loss is 28.44493865966797\n",
      "Current training loss is 28.444021224975586\n",
      "Current training loss is 28.443092346191406\n",
      "Current training loss is 28.44217300415039\n",
      "Current training loss is 28.441246032714844\n",
      "Current training loss is 28.44033432006836\n",
      "Current training loss is 28.439420700073242\n",
      "Current training loss is 28.438507080078125\n",
      "Current training loss is 28.437589645385742\n",
      "Current training loss is 28.436681747436523\n",
      "Current training loss is 28.43577003479004\n",
      "Current training loss is 28.43486785888672\n",
      "Current training loss is 28.433961868286133\n",
      "Current training loss is 28.43305778503418\n",
      "Current training loss is 28.43215560913086\n",
      "Current training loss is 28.431249618530273\n",
      "Current training loss is 28.430355072021484\n",
      "Current training loss is 28.42945671081543\n",
      "Current training loss is 28.428564071655273\n",
      "Current training loss is 28.42767333984375\n",
      "Current training loss is 28.426776885986328\n",
      "Current training loss is 28.425888061523438\n",
      "Current training loss is 28.42500114440918\n",
      "Current training loss is 28.424118041992188\n",
      "Current training loss is 28.42323112487793\n",
      "Current training loss is 28.42234992980957\n",
      "Current training loss is 28.421464920043945\n",
      "Current training loss is 28.42058563232422\n",
      "Current training loss is 28.419706344604492\n",
      "Current training loss is 28.418832778930664\n",
      "Current training loss is 28.41796112060547\n",
      "Current training loss is 28.41708755493164\n",
      "Current training loss is 28.416215896606445\n",
      "Current training loss is 28.415340423583984\n",
      "Current training loss is 28.414474487304688\n",
      "Current training loss is 28.41360855102539\n",
      "Current training loss is 28.41274642944336\n",
      "Current training loss is 28.411884307861328\n",
      "Current training loss is 28.411020278930664\n",
      "Current training loss is 28.410167694091797\n",
      "Current training loss is 28.409303665161133\n",
      "Current training loss is 28.408451080322266\n",
      "Current training loss is 28.407594680786133\n",
      "Current training loss is 28.406740188598633\n",
      "Current training loss is 28.40589141845703\n",
      "Current training loss is 28.405040740966797\n",
      "Current training loss is 28.404197692871094\n",
      "Current training loss is 28.403348922729492\n",
      "Current training loss is 28.402503967285156\n",
      "Current training loss is 28.40166473388672\n",
      "Current training loss is 28.40082550048828\n",
      "Current training loss is 28.399980545043945\n",
      "Current training loss is 28.399147033691406\n",
      "Current training loss is 28.3983097076416\n",
      "Current training loss is 28.39747428894043\n",
      "Current training loss is 28.396644592285156\n",
      "Current training loss is 28.39581298828125\n",
      "Current training loss is 28.394981384277344\n",
      "Current training loss is 28.39415740966797\n",
      "Current training loss is 28.393327713012695\n",
      "Current training loss is 28.392507553100586\n",
      "Current training loss is 28.391679763793945\n",
      "Current training loss is 28.39086151123047\n",
      "Current training loss is 28.390043258666992\n",
      "Current training loss is 28.389223098754883\n",
      "Current training loss is 28.388404846191406\n",
      "Current training loss is 28.387590408325195\n",
      "Current training loss is 28.386775970458984\n",
      "Current training loss is 28.38596534729004\n",
      "Current training loss is 28.385160446166992\n",
      "Current training loss is 28.384347915649414\n",
      "Current training loss is 28.38353729248047\n",
      "Current training loss is 28.382741928100586\n",
      "Current training loss is 28.381935119628906\n",
      "Current training loss is 28.38113021850586\n",
      "Current training loss is 28.380332946777344\n",
      "Current training loss is 28.379533767700195\n",
      "Current training loss is 28.37873649597168\n",
      "Current training loss is 28.37794303894043\n",
      "Current training loss is 28.377145767211914\n",
      "Current training loss is 28.376359939575195\n",
      "Current training loss is 28.375566482543945\n",
      "Current training loss is 28.37478256225586\n",
      "Current training loss is 28.373991012573242\n",
      "Current training loss is 28.373205184936523\n",
      "Current training loss is 28.372421264648438\n",
      "Current training loss is 28.37163543701172\n",
      "Current training loss is 28.370859146118164\n",
      "Current training loss is 28.37007713317871\n",
      "Current training loss is 28.36929702758789\n",
      "Current training loss is 28.36852264404297\n",
      "Current training loss is 28.367746353149414\n",
      "Current training loss is 28.366971969604492\n",
      "Current training loss is 28.36619758605957\n",
      "Current training loss is 28.365434646606445\n",
      "Current training loss is 28.36466407775879\n",
      "Current training loss is 28.363901138305664\n",
      "Current training loss is 28.363134384155273\n",
      "Current training loss is 28.362367630004883\n",
      "Current training loss is 28.361608505249023\n",
      "Current training loss is 28.360849380493164\n",
      "Current training loss is 28.36008644104004\n",
      "Current training loss is 28.359331130981445\n",
      "Current training loss is 28.35857582092285\n",
      "Current training loss is 28.35782241821289\n",
      "Current training loss is 28.357065200805664\n",
      "Current training loss is 28.35630989074707\n",
      "Current training loss is 28.355566024780273\n",
      "Current training loss is 28.354816436767578\n",
      "Current training loss is 28.35407257080078\n",
      "Current training loss is 28.35332489013672\n",
      "Current training loss is 28.35257911682129\n",
      "Current training loss is 28.351835250854492\n",
      "Current training loss is 28.35109519958496\n",
      "Current training loss is 28.350360870361328\n",
      "Current training loss is 28.34962272644043\n",
      "Current training loss is 28.348886489868164\n",
      "Current training loss is 28.348154067993164\n",
      "Current training loss is 28.347415924072266\n",
      "Current training loss is 28.346689224243164\n",
      "Current training loss is 28.34595489501953\n",
      "Current training loss is 28.345232009887695\n",
      "Current training loss is 28.34449577331543\n",
      "Current training loss is 28.343780517578125\n",
      "Current training loss is 28.343055725097656\n",
      "Current training loss is 28.342329025268555\n",
      "Current training loss is 28.34160614013672\n",
      "Current training loss is 28.34088897705078\n",
      "Current training loss is 28.340167999267578\n",
      "Current training loss is 28.339452743530273\n",
      "Current training loss is 28.338735580444336\n",
      "Current training loss is 28.338024139404297\n",
      "Current training loss is 28.337308883666992\n",
      "Current training loss is 28.33659553527832\n",
      "Current training loss is 28.335887908935547\n",
      "Current training loss is 28.335182189941406\n",
      "Current training loss is 28.334476470947266\n",
      "Current training loss is 28.333776473999023\n",
      "Current training loss is 28.333070755004883\n",
      "Current training loss is 28.33237075805664\n",
      "Current training loss is 28.331668853759766\n",
      "Current training loss is 28.330970764160156\n",
      "Current training loss is 28.330270767211914\n",
      "Current training loss is 28.32957649230957\n",
      "Current training loss is 28.328880310058594\n",
      "Current training loss is 28.32818603515625\n",
      "Current training loss is 28.32749366760254\n",
      "Current training loss is 28.326805114746094\n",
      "Current training loss is 28.326120376586914\n",
      "Current training loss is 28.3254337310791\n",
      "Current training loss is 28.324750900268555\n",
      "Current training loss is 28.324064254760742\n",
      "Current training loss is 28.32337760925293\n",
      "Current training loss is 28.322696685791016\n",
      "Current training loss is 28.3220157623291\n",
      "Current training loss is 28.321334838867188\n",
      "Current training loss is 28.320663452148438\n",
      "Current training loss is 28.319984436035156\n",
      "Current training loss is 28.319313049316406\n",
      "Current training loss is 28.31863784790039\n",
      "Current training loss is 28.317970275878906\n",
      "Current training loss is 28.317298889160156\n",
      "Current training loss is 28.316633224487305\n",
      "Current training loss is 28.315963745117188\n",
      "Current training loss is 28.31529426574707\n",
      "Current training loss is 28.31463050842285\n",
      "Current training loss is 28.31397247314453\n",
      "Current training loss is 28.313310623168945\n",
      "Current training loss is 28.31264877319336\n",
      "Current training loss is 28.311988830566406\n",
      "Current training loss is 28.31133270263672\n",
      "Current training loss is 28.310678482055664\n",
      "Current training loss is 28.310026168823242\n",
      "Current training loss is 28.309370040893555\n"
     ]
    }
   ],
   "source": [
    "for i in range(3000):\n",
    "    sess.run(opt)\n",
    "    print(\"Current training loss is {}\".format(sess.run(loss)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0xb38699ac8>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "preds = sess.run(ys_pred)\n",
    "plt.figure()\n",
    "plt.plot(xs, preds, label='prediction')\n",
    "plt.plot(xs, ys, label='target')\n",
    "plt.legend()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
